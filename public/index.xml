<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Berent Å. S. Lunde on Berent Å. S. Lunde</title>
    <link>/</link>
    <description>Recent content in Berent Å. S. Lunde on Berent Å. S. Lunde</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The fear index, VIX and variance swaps</title>
      <link>/post/the-fear-index-vix-and-variance-swaps/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/the-fear-index-vix-and-variance-swaps/</guid>
      <description>


&lt;p&gt;Four years ago a friend from HKUST, that had started working in the finance industry, back in 2014 asked me about the VIX. He could not figure out why the CBOE VIX volatility index, popularly called the fear index, was calculated the way it was (see this &lt;a href=&#34;https://www.cboe.com/micro/vix/vixwhite.pdf&#34;&gt;white paper&lt;/a&gt;). Pursuing the quest of understanding the VIX, I remember it felt like very few people truly understood what was going on, and that the ones that did had no interest in sharing their knowledge.&lt;/p&gt;
&lt;p&gt;After lots of reading I figured it out. Here is what I was able to collect:&lt;/p&gt;
&lt;div id=&#34;variance-swap&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Variance Swap&lt;/h2&gt;
&lt;p&gt;Before we start the calculations it should be noted that: At maturity, the payoff of a variance swap is &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
10000N(\Sigma^2-K_{var})
\end{align*}
\]&lt;/span&gt; where&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; = notational amount quoted in $ per volatility point squared (hence the factor of &lt;span class=&#34;math inline&#34;&gt;\(100^2=10000\)&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\Sigma^2\)&lt;/span&gt; = realized variance in the underlying asset during the life of the swap.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(K_{var}\)&lt;/span&gt; = strike on variance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The underlying assumption is that the S&amp;amp;P500, &lt;span class=&#34;math inline&#34;&gt;\(S_t\)&lt;/span&gt;, follows a geometric brownian motion&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
dS_t&amp;amp;=\mu_tS_tdt+\sigma_tS_tdB_t.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is usually set to be the short rate &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;This is probably the most famous stochastic differential equation which we solve using Itô’s lemma (this is probably trivial to you, but I don’t know how much you know so please don’t be offended that I’m showing this).\&lt;/p&gt;
&lt;p&gt;Use the substitution &lt;span class=&#34;math inline&#34;&gt;\(Y_t=\log (S_t)\)&lt;/span&gt; and apply It^o’s Lemma to the function &lt;span class=&#34;math inline&#34;&gt;\(f(t,x):=\log (x)\)&lt;/span&gt;. The partial derivatives are&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
\frac{\partial f}{\partial t}(t,X_t)=0,\;
\frac{\partial f}{\partial x}(t,X_t)=\frac{1}{S_t},\;
\frac{\partial^2 f}{\partial^2 x}(t,X_t)=-\frac{1}{S_t^2}. 
\end{align}
\]&lt;/span&gt; Thus we obtain &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
dY_t&amp;amp;=\left(\frac{\mu_tS_t}{S_t}-\frac{\sigma_t^2S_t^2}{2S_t^2}\right)dt+
\frac{\sigma_tS_t}{S_t}dB_t \text{ by It\^o&amp;#39;s Lemma} \\
&amp;amp;=\left(\mu_t-\frac{\sigma_t^2}{2}\right)dt+\sigma_tdB_t
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Dividing the equation for &lt;span class=&#34;math inline&#34;&gt;\(dS_t\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(S_t\)&lt;/span&gt; and subtracting this last equation we obtain:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\frac{dS_t}{S_t}-d(\log(S_t))&amp;amp;=\mu_tdt+\sigma_tdB_t-\left(\left(\mu_t-\frac{\sigma_t^2}{2}\right)dt+\sigma_tdB_t\right)\\
&amp;amp;=\frac{\sigma_t^2}{2}dt.
\end{align*}
\]&lt;/span&gt; Hence, &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
Variance = \frac{1}{T}\int_{0}^{T}\sigma_t^2dt=\frac{2}{T}\left(\int_{0}^{T}\frac{dS_t}{S_t}-\log(\frac{S_T}{S_0})\right).
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Notice that &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\log\left(\frac{S_T}{S_0}\right)=\log\left(\frac{S_T}{S^*}\right)+\log\left(\frac{S^*}{S_0}\right)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We now make use of the dirac delta function (&lt;span class=&#34;math inline&#34;&gt;\(\delta(x)=\infty\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(x=0\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt; otherwise) to write on &lt;span class=&#34;math inline&#34;&gt;\(-\log\left(\frac{S_T}{S_*}\right)\)&lt;/span&gt; and then integrate by parts twice (this is where the &lt;span class=&#34;math inline&#34;&gt;\(1/K^2\)&lt;/span&gt; comes from):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
-\log\left(\frac{S_T}{S_*}\right)&amp;amp;=\log (S^*)-\log (S_T)\\
&amp;amp;=\log (S^*)-\int_{o}^{\infty}log(K)\delta(S_T-K)dK\text{ by propertis of the dirac delta function}\\
&amp;amp;=\log (S^*)-\int_{o}^{S^*}log(K)\delta(S_T-K)dK-\int_{S^*}^{\infty}log(K)\delta(S_T-K)dK \text{ splitting the integral}\\
&amp;amp;=\log (S^*)-\left[\log(K)1_{(S_T&amp;lt;K)}\right]_0^{S^*}+\int_{0}^{S^*}\frac{1}{K}1_{(S_T&amp;lt;K)}dK\\
&amp;amp;-\left[\log(K)1_{(S_T\geq K)}\right]_{S^*}^{\infty}+\int_{S^*}^{\infty}\frac{1}{K}1_{(S_T\geq K)}dK\text{ by integration by parts}\\
&amp;amp;=\log (S^*)- \log(S^*)1_{(S_T&amp;lt;K)}+\left.\frac{1}{K}[K-S_T]^+\right|_0^{S^*}+\int_{0}^{S^*}\frac{1}{K^2}[K-S_T]^+dK\\
&amp;amp;-\log(S^*)1_{(S_T\geq K)}+\left.\frac{1}{K}[S_T-K]^+\right|_{S^*}^\infty+\int_{S^*}^{\infty}\frac{1}{K^2}[S_T-K]^+dK\\
&amp;amp;=\frac{1}{S^*}\left(\left[S^*-S_T\right]^+-\left[S_T-S^*\right]^+\right)
+\int_{0}^{S^*}\frac{1}{K^2}[K-S_T]^+dK+\int_{S^*}^{\infty}\frac{1}{K^2}[S_T-K]^+dK\\
&amp;amp;=\frac{1}{S^*}\left(S^*-S_T\right)
+\int_{0}^{S^*}\frac{1}{K^2}[K-S_T]^+dK+\int_{S^*}^{\infty}\frac{1}{K^2}[S_T-K]^+dK
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This was kinda tedious, but this calculation is as noted before the reason for &lt;span class=&#34;math inline&#34;&gt;\(1/K^2\)&lt;/span&gt; part in the final equation.\&lt;/p&gt;
&lt;p&gt;We now have the final equation for the variance&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
Variance=\frac{2}{T}\left(\int_{0}^{T}\frac{dS_t}{S_t}-\log(\frac{S^*}{S_0})
+\frac{1}{S^*}\left(S^*-S_T\right)
+\int_{0}^{S^*}\frac{1}{K^2}[K-S_T]^+dK+\int_{S^*}^{\infty}\frac{1}{K^2}[S_T-K]^+dK
\right)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Just realizing I should have said that &lt;span class=&#34;math inline&#34;&gt;\(Variance = \Sigma^2\)&lt;/span&gt; (realized variance in the underlying asset during the life of the swap). To avoid arbitrage in the payoff equation (equation located in the start) we have to set: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
E[\Sigma^2]=K_{var},
\end{align*}
\]&lt;/span&gt; the expectation of the realized variance is equal to the strike on the variance.&lt;/p&gt;
&lt;p&gt;For the expecattions, not that &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
E\left[\int_{0}^{T}\frac{dS_t}{S_t}\right]&amp;amp;=E\left[\int_{0}^{T}\mu_tdt+\int_{0}^{T}\sigma_tdB_t\right]=\mu T \text{ assuming }\mu\text{ is constant}\\
E\left[\log(\frac{S^*}{S_0})\right]&amp;amp;=\log(\frac{S^*}{S_0})\text{ they&amp;#39;re just constants}\\
E\left[S_T\right]&amp;amp;=S_0e^{\mu T}\\
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Also observe that the price of a put and a call option both with strike &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
P(K)&amp;amp;=e^{-eT}E\left[\left(K-S_T\right)^+\right]\\
C(K)&amp;amp;=e^{-eT}E\left[\left(S_T-K\right)^+\right]\\
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Taking expectations of the equation for the realized variance we thus obtain&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
K_{var}=E[Variance]=\frac{2}{T}\left(\mu T-\left(\frac{S_O}{S^*}e^{\mu T}-1\right)
-\log\left(\frac{S^*}{S_0}\right)
+e^{\mu T}
\int_{0}^{S^*}\frac{P(K)}{K^2}dK+e^{\mu T}\int_{S^*}^{\infty}\frac{C(K)}{K^2}dK
\right)
\end{align*}
\]&lt;/span&gt; Another way of writing this is: &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
K_{var}&amp;amp;=\frac{2}{T}\left(\log\left(\frac{F_o}{S^*}\right)-\left(\frac{F_0}{S^*}-1\right)+e^{\mu T}
\int_{0}^{S^*}\frac{P(K)}{K^2}dK+e^{\mu T}\int_{S^*}^{\infty}\frac{C(K)}{K^2}dK
\right)
\end{align*}
\]&lt;/span&gt; Now, making the smart choice of setting &lt;span class=&#34;math inline&#34;&gt;\(S^*=F_0=S_0e^{\mu T}\)&lt;/span&gt; a lot cancel out and we have the final equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
K_{var}=\frac{2e^{\mu T}}{T}\left[\int_{0}^{F_0}\frac{P(K)}{K^2}dK+\int_{F_0}^{\infty}\frac{C(K)}{K^2}dK\right].
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-variance-swap-and-the-vix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between variance swap and the VIX}&lt;/h2&gt;
&lt;p&gt;We now want to make a discretized version (the VIX) of our fair price variance swap formula. Assume that the price of options with strike prices &lt;span class=&#34;math inline&#34;&gt;\(K_i,\,(i=1:n)\)&lt;/span&gt; are known and that &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
K_1&amp;lt;K_2&amp;lt;...&amp;lt;K_n
\end{align*}
\]&lt;/span&gt; abd choose &lt;span class=&#34;math inline&#34;&gt;\(S^*\)&lt;/span&gt; equal to the first strike price below &lt;span class=&#34;math inline&#34;&gt;\(F_0\)&lt;/span&gt;, define the function&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
Q(K_i)&amp;amp;=P(K_i)1_{(K_i\leq S^*)}+C(K_i)1_{(S^*&amp;lt;K_i)},
\end{align*} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;then approximate the integrals as following&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
e^{\mu T}
\int_{0}^{S^*}\frac{P(K)}{K^2}dK+e^{\mu T}\int_{S^*}^{\infty}\frac{C(K)}{K^2}dK=e^{\mu T}\sum_{i=1}^{n}\frac{\Delta K_i}{K_i^2}Q(K_i)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\Delta K_i&amp;amp;=\frac{K_{i+1}-K_{i}}{2},\,\,i=2:n-1\\
\Delta K_1&amp;amp;=K_2-K_1\\
\Delta K_n&amp;amp;=K_n-K_{n-1}.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Set this aproximation inside the equation for fair variance swap with and we obtain &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
K_{var}\approx\frac{2}{T}\left(\log\left(\frac{F_o}{S^*}\right)-\left(\frac{F_0}{S^*}-1\right)+e^{\mu T}\sum_{i=1}^{n}\frac{\Delta K_i}{K_i^2}Q(K_i)\right)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The maclaurin polynomial for &lt;span class=&#34;math inline&#34;&gt;\(\log(F_0/S^*)\)&lt;/span&gt; is &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\log\left(\frac{F_0}{S^*}\right)&amp;amp;=\left(\frac{F_0}{S^*}-1\right)-\frac{1}{2}\left(\frac{F_0}{S^*}-1\right)^2+O\left(\left(\frac{F_0}{S^*}-1\right)^2\right)
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus &lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
\log\left(\frac{F_o}{S^*}\right)-\left(\frac{F_0}{S^*}-1\right)
=-\frac{1}{2}\left(\frac{F_0}{S^*}-1\right)^2+O\left(\left(\frac{F_0}{S^*}-1\right)^2\right),
\end{align*}
\]&lt;/span&gt; and we obtain our final approximation&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{align*}
K_{var}\approx\frac{2}{T}e^{\mu T}\sum_{i=1}^{n}\frac{\Delta K_i}{K_i^2}Q(K_i)-\frac{1}{2}\left(\frac{F_0}{S^*}-1\right)^2
\end{align*}
\]&lt;/span&gt; which we recognize as the fomula for the VIX.&lt;/p&gt;
&lt;p&gt;This is what I’m able to write for tonight, there is a party that I kinda want to go to, also it is 9pm. I hope this helps! Good luck with everything Charles. I’ll probably write something more over the weekend about VIX and then how to estimate VIX well.&lt;/p&gt;
&lt;p&gt;As I said before, remember that the underlying “stock” here is the S&amp;amp;P500 index! I am note sure if there are call and put options that has the S&amp;amp;P as underlying, if there isn’t, you should use underlying stocks that is highly correlated (beta) with the S&amp;amp;P and calculate the VIX for all of them and maybe take the average. Something like that, I’ll definetly come back to that when trying to come up with a good estimator. One of my professors also proposed a model for making predictions of the VIX, this is something that is surely interesting if it works.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Nordic QuantLab</title>
      <link>/project/nordic-quantlab/nql_project/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0100</pubDate>
      
      <guid>/project/nordic-quantlab/nql_project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SPI</title>
      <link>/project/exactspa/exactspa/</link>
      <pubDate>Mon, 12 Nov 2018 00:00:00 +0100</pubDate>
      
      <guid>/project/exactspa/exactspa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0200</pubDate>
      
      <guid>/tutorial/example/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AquaLens</title>
      <link>/project/aqualens/aqualens/</link>
      <pubDate>Sat, 12 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/project/aqualens/aqualens/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Information efficient gradient tree boosting</title>
      <link>/talk/bergen-ml-meetup-2018-10-31/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0100</pubDate>
      
      <guid>/talk/bergen-ml-meetup-2018-10-31/</guid>
      <description>&lt;p&gt;A repository for the presentation can be found at &lt;a href=&#34;https://github.com/bergen-ml/2018-10-31-lunde&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Person Re-Identification System For Mobile Devices</title>
      <link>/publication/person-re-id/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0200</pubDate>
      
      <guid>/publication/person-re-id/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mobile visual clothing search</title>
      <link>/publication/clothing-search/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0200</pubDate>
      
      <guid>/publication/clothing-search/</guid>
      <description>&lt;p&gt;More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information efficient gradient tree boosting</title>
      <link>/slides/mlmeetup/mlmeetup_presentation_bl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/slides/mlmeetup/mlmeetup_presentation_bl/</guid>
      <description>


&lt;p&gt;  &lt;/p&gt;
&lt;div id=&#34;outline&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Outline&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Background
&lt;ul&gt;
&lt;li&gt;Supervised learning&lt;/li&gt;
&lt;li&gt;Gradient boosting&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Information efficient GTB
&lt;ul&gt;
&lt;li&gt;Frequency domain&lt;/li&gt;
&lt;li&gt;Japanese tricks&lt;/li&gt;
&lt;li&gt;Bottoms up&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Experimental illustrations
&lt;ul&gt;
&lt;li&gt;Bias&lt;/li&gt;
&lt;li&gt;Convergence&lt;/li&gt;
&lt;li&gt;Comparisons&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Recap and possibilities&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;background&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;div id=&#34;supervised-learning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Supervised learning&lt;/h2&gt;
&lt;div class=&#34;columns-2&#34;&gt;
&lt;p&gt;&lt;img src=&#34;figures/isl_regression.png&#34; width=&#34;400px&#34; height=&#34;400px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/loss_vs_complexity.png&#34; width=&#34;400px&#34; height=&#34;400px&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;supervised-learning-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Supervised learning&lt;/h2&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Mapping &lt;span class=&#34;math inline&#34;&gt;\(f:A \rightarrow B\)&lt;/span&gt; using a finite dataset.&lt;/li&gt;
&lt;li&gt;What is a good model?&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;A model that generalises well to unseen data.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Generalises&lt;/em&gt;: average over new data&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;well&lt;/em&gt;: minimises Loss&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\tilde{f} = \arg\min_f {\mathbb{E}}_{\hat{w}}{\mathbb{E}}_{{\mathbf{x}},{y}}\left[{\mathcal{L}}\left({y}, f({\mathbf{x}}; \hat{w})\right)\right]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-boosting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gradient boosting&lt;/h2&gt;
&lt;p&gt;Algorithm:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Initialize model with a constant value: &lt;span class=&#34;math inline&#34;&gt;\(F_0(x) = \underset{\gamma}{\arg\min} \sum_{i=1}^n {\mathcal{L}}(y_i, \gamma).\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;For &lt;span class=&#34;math inline&#34;&gt;\(m = 1\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;:
&lt;ol style=&#34;list-style-type: lower-roman&#34;&gt;
&lt;li&gt;Compute so-called ‘’pseudo-residuals’’: &lt;span class=&#34;math inline&#34;&gt;\(r_{im} = -\left[\frac{\partial {\mathcal{L}}(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)} \quad \mbox{for } i=1,\ldots,n.\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Fit a base learner (e.g. tree) &lt;span class=&#34;math inline&#34;&gt;\(h_m(x)\)&lt;/span&gt; to pseudo-residuals, i.e. train it using the training set &lt;span class=&#34;math inline&#34;&gt;\(\{(x_i, r_{im})\}_{i=1}^n\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Compute multiplier &lt;span class=&#34;math inline&#34;&gt;\(\gamma_m\)&lt;/span&gt; by solving the following one-dimensional optimization problem: &lt;span class=&#34;math inline&#34;&gt;\(\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n {\mathcal{L}}\left(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)\right)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Update the model: &lt;span class=&#34;math inline&#34;&gt;\(F_m(x) = F_{m-1}(x) + \gamma_m h_m(x).\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;li&gt;Output &lt;span class=&#34;math inline&#34;&gt;\(F_M(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;gradient-tree-boosting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Gradient tree boosting&lt;/h2&gt;
&lt;p&gt;At iteration &lt;span class=&#34;math inline&#34;&gt;\(\small m+1\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Computes &lt;span class=&#34;math inline&#34;&gt;\(\small g_i=-r_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\small h_i=\left[\frac{\partial^2 {\mathcal{L}}(y_i, F(x_i))}{\partial F(x_i)^2}\right]_{F(x)=F_{m-1}(x)}\quad \mbox{for } i=1,\ldots,n.\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Approximates &lt;span class=&#34;math inline&#34;&gt;\({\mathcal{L}}\)&lt;/span&gt; by a second order approximation about &lt;span class=&#34;math inline&#34;&gt;\(F_{m}\)&lt;/span&gt;: &lt;span class=&#34;math display&#34;&gt;\[{\small
{\mathbb{E}}_{{\mathbf{x}},{y}}\left[ {\mathcal{L}}({y}, (f_m+f)({\mathbf{x}}) \right] 
\approx
\frac{1}{n}\sum_{i=1}^{n} 
{\mathcal{L}}(y_i,f_m({\mathbf{x}}_i))+g_i f({\mathbf{x}}_i)
+\frac{1}{2}h_if({\mathbf{x}}_i)^2
}\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Solves the quadratic problem exactly: &lt;span class=&#34;math inline&#34;&gt;\(\small \hat{w} = -\frac{\sum g_i}{\sum h_i}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\small {\mathcal{L}}(y,\hat{w}) = -\frac{\left(\sum g_i\right)^2}{2\sum h_i}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Splits nodes to maximize loss reduction:
&lt;center&gt;
&lt;span class=&#34;math inline&#34;&gt;\({\small \frac{1}{2}\left[\frac{\left(\sum_{i\in I_L(j,s)}g_i\right)^2}{\sum_{i\in I_L(j,s)}h_i} + \frac{\left(\sum_{i\in I_R(j,s)}g_i\right)^2}{\sum_{i\in I_R(j,s)}h_i} -\frac{\left(\sum_{i\in I_k}g_i\right)^2}{\sum_{i\in I_k}h_i}\right] }\)&lt;/span&gt;
&lt;/center&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The problem&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Wrong objective:&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\({\mathcal{L}}(y,\hat{w}) = -\frac{\left(\sum g_i\right)^2}{2\sum h_i}\)&lt;/span&gt; is not an unbiased estimator of &lt;span class=&#34;math inline&#34;&gt;\({\mathbb{E}}_{\hat{w}}{\mathbb{E}}_{{\mathbf{x}},{y}}\left[{\mathcal{L}}\left({y}, f({\mathbf{x}}; \hat{w})\right)\right]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Regularization: &lt;span class=&#34;math inline&#34;&gt;\({\mathcal{L}}+ \Omega(w)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;XGBoost and LightGBM each have &amp;gt; 10 hyperparameters that should be configured.&lt;/li&gt;
&lt;li&gt;High dimensional optimisation is difficult.&lt;/li&gt;
&lt;li&gt;Time consuming.&lt;/li&gt;
&lt;li&gt;Requires human expertiese.&lt;/li&gt;
&lt;li&gt;Cross validation is information inefficient.
&lt;ul&gt;
&lt;li&gt;Variable results.&lt;/li&gt;
&lt;li&gt;Risky in terms of over and underfitting.&lt;/li&gt;
&lt;li&gt;Kaggle mantra: trust your local validation…&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;information-efficient-gradient-tree-boosting&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Information efficient gradient tree boosting&lt;/h1&gt;
&lt;div id=&#34;retrieval-of-the-estimator-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Retrieval of the estimator distribution&lt;/h2&gt;
&lt;p&gt;For model selection purposes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Seek the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\hat{w} = -\frac{\sum g_i}{\sum h_i}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;However:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Distribution is unknown unless prior information about the data-generating process is known.&lt;/li&gt;
&lt;li&gt;Asymptotic theory assumptions does not hold in tree-leaves.&lt;/li&gt;
&lt;li&gt;Bootstrapping to estimate the empirical/ resample distribution is extremely costly.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;But we know the probability of resampling one observation event: &lt;span class=&#34;math inline&#34;&gt;\(\frac{1}{n}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;We also know the one-observation resample events characteristic function (Fourier transform): &lt;span class=&#34;math display&#34;&gt;\[\small 
{\varphi^*}_{g,h}(u,v) = \frac{1}{n} \sum_{j=1}^n e^{iug_j + ivh_j}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-frequency-domain&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The frequency domain&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The frequency domain is a fantastic place to combine randomness&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The characteristic function takes a frequency, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, as argument: &lt;span class=&#34;math display&#34;&gt;\[\small
        \varphi_X(s) = {\mathbb{E}}\left[e^{isx}\right] = \int e^{isx}p_X(x)~dx\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; is the &lt;em&gt;angular frequency&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(s=2\pi \tau\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; is the number of rotations per unit of time.&lt;/li&gt;
&lt;li&gt;The average of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; wrapped around the unit circle with &lt;em&gt;angular frequency&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\varphi_X(s)\)&lt;/span&gt; is called the &lt;em&gt;Fourier transform&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(p_X(x)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-frequency-domain-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The frequency domain&lt;/h2&gt;
&lt;p&gt;The characteristic function takes a frequency, &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, as argument: &lt;span class=&#34;math display&#34;&gt;\[ \small
        \varphi_X(s) = {\mathbb{E}}\left[e^{isx}\right] = \int e^{isx}p_X(x)~dx\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The average of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; wrapped around the unit circle with &lt;em&gt;angular frequency&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. &lt;img src=&#34;figures/uniform_cf_1.png&#34; width=&#34;550px&#34; height=&#34;370px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;property-of-the-characteristic-function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Property of the characteristic function&lt;/h2&gt;
&lt;p&gt;General property, if &lt;span class=&#34;math inline&#34;&gt;\(\small X_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\small X_2\)&lt;/span&gt; are independent, then &lt;span class=&#34;math display&#34;&gt;\[\small
        \varphi_{\left(X_1+X_2\right)}(s) = \varphi_{X_1}(s) \times \varphi_{X_2}(s).\]&lt;/span&gt; Thus &lt;span class=&#34;math display&#34;&gt;\[
\begin{align}\label{eq empirical cf}
\varphi_{\sum g,\sum h}^*(u,v) = \left[ \varphi_{g, h}^*\left(u,v\right)  \right]^n.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can obtain the density and distribution functions: &lt;span class=&#34;math display&#34;&gt;\[\small
    \begin{align*}
    p_X(x) &amp;amp;= \frac{1}{2\pi} \int_{-\infty}^\infty
    \varphi_X(s)e^{-isx} ds.
    \end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/tikz_frequency.png&#34; width=&#34;450px&#34; height=&#34;200px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-the-actual-problem&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Back to the actual problem&lt;/h2&gt;
&lt;p&gt;What we have: training error – the error on the data we trained on &lt;span class=&#34;math display&#34;&gt;\[\small
{\mathbb{E}}_{{\mathbf{x}},{y}}\left[ {\mathcal{L}}({y},F({\mathbf{x}};\hat{w}({\mathbf{x}},{y})))\right].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What we need: test error – the error on previously unseen data &lt;span class=&#34;math display&#34;&gt;\[\small
{\mathbb{E}}_{\hat{w}}{\mathbb{E}}_{{\mathbf{x}},{y}}\left[{\mathcal{L}}\left({y}, F({\mathbf{x}}; \hat{w})\right)\right].\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;How to get it?&lt;/p&gt;
&lt;p&gt;This is a known problem!&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;citation&#34;&gt;Akaike (1974)&lt;/span&gt;: log-likelihood, asymptotics, true model is in the modelspace.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;citation&#34;&gt;Takeuchi (1976)&lt;/span&gt;: generalises AIC – true model not necessarily in the modelspace.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;citation&#34;&gt;Murata, Yoshizawa, and Amari (1994)&lt;/span&gt;: generalises TIC – differentiable loss function&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;citation&#34;&gt;Konishi and Kitagawa (1996)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;citation&#34;&gt;Shimodaira and Maeda (2018)&lt;/span&gt;: generalises TIC to missing data&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;an-information-criterion-for-gtb&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An information criterion for GTB&lt;/h2&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span id=&#34;fig:unnamed-chunk-7&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;figures/tikz_japanese.png&#34; alt=&#34;Geometric view of Japanese tricks&#34; width=&#34;400px&#34; height=&#34;200px&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Geometric view of Japanese tricks
&lt;/p&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;Avoids asymptotics by utilizing the frequency domain.&lt;/li&gt;
&lt;li&gt;Exploit the iterative structure of gradient boosting and the locally constant structure of trees.&lt;/li&gt;
&lt;li&gt;Avoids high-dimensional matrix inversion (for possibly non-positive definite matrices). &lt;span class=&#34;math display&#34;&gt;\[\small
\begin{align}\label{eq criterion exact}
{\mathbb{E}}_{f_m}{\mathbb{E}}_{{y}} \left[   \hat{{\mathcal{L}}}(y,\hat{f}_m)  \right]
=
{\mathbb{E}}_{{y}} \left[   \hat{{\mathcal{L}}}(y,\hat{f}_m)  \right]
&amp;amp;+  \frac{1}{2} \sum_{k=1}^{T}{\mathbb{E}}_{{y}} \left[     h| q({\mathbf{x}})=k \right  ]{\mathrm{Var}}[w_k]\notag\\
&amp;amp;   +   \frac{1}{2} 
\sum_{k=1}^{T}{\mathbb{E}}_{{y}} \left[ \left\{ h| q({\mathbf{x}})=k \right\} (w_0 - \hat{w}_k)^2 \right].
\end{align}\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;bottoms-up-branch-pruning&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bottoms up: branch pruning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;figures/algo_bottomsup.png&#34; width=&#34;500px&#34; height=&#34;250px&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;figures/tikz_bottomsup.png&#34; width=&#34;700px&#34; height=&#34;250px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;experimental-illustrations&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Experimental illustrations&lt;/h1&gt;
&lt;div id=&#34;bias&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bias&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;figures/Criterion_bias.png&#34; width=&#34;600px&#34; height=&#34;300px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Focus on the individual bias in a node.&lt;/li&gt;
&lt;li&gt;Repeated simulation experiment to obtain the distribution of bias estimators.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Cross validation is information inefficient.&lt;/li&gt;
&lt;li&gt;Asymptotic criteria, NIC, is biased downward.&lt;/li&gt;
&lt;li&gt;LIC is slightly biased upwards.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;convergence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Convergence&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;figures/gtb_loss_convergence.png&#34; width=&#34;600px&#34; height=&#34;300px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Training, testing, and estimated test-loss versus number of trees added.&lt;/li&gt;
&lt;li&gt;Number of leaves in n’th tree.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Convergence in number of trees: The estimted test-loss minimum correspond to the testing loss.&lt;/li&gt;
&lt;li&gt;The number of leaves decreases as information is learned by the model.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;comparisons-on-titanic-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparisons on Titanic data&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;figures/Model_loss_n30.png&#34; width=&#34;600px&#34; height=&#34;300px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Comparisons versus a regularized model and one trained with a validation dataset (30% of training).&lt;/li&gt;
&lt;li&gt;Repeated random splitting in training and test datasets.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Our information efficient approach is slightly better overall (mean).&lt;/li&gt;
&lt;li&gt;The standard deviation is the lowest for our approach (consistency and information efficient).&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;recap-and-possibilities&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Recap and possibilities&lt;/h1&gt;
&lt;div id=&#34;two-different-approaches&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Two different approaches&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Criteria&lt;/th&gt;
&lt;th&gt;Regularized&lt;/th&gt;
&lt;th&gt;Information efficient&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Speed:&lt;/td&gt;
&lt;td&gt;&lt;span class=&#34;math inline&#34;&gt;\(10\times\)&lt;/span&gt; number of grid points&lt;/td&gt;
&lt;td&gt;One run&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Automated:&lt;/td&gt;
&lt;td&gt;No&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;User knowledge:&lt;/td&gt;
&lt;td&gt;Some&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Risks:&lt;/td&gt;
&lt;td&gt;Over and underfitting&lt;/td&gt;
&lt;td&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Optimal:&lt;/td&gt;
&lt;td&gt;Close&lt;/td&gt;
&lt;td&gt;Yes&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;new-possibilities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;New possibilities&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Users without deep ML knowledge. &lt;img src=&#34;figures/data_science_1.png&#34; width=&#34;200px&#34; height=&#34;200px&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;Build lots of models, very fast.
&lt;ul&gt;
&lt;li&gt;Andrew Ng: “Anything a typical person can do with one second of thought, we can probably now or soon automate.”&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Online learning.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Inner optimization routines (Gaussian Markov random fields etc.).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-akaike1974new&#34;&gt;
&lt;p&gt;Akaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” &lt;em&gt;IEEE Transactions on Automatic Control&lt;/em&gt; 19 (6). Ieee: 716–23.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-konishi1996generalised&#34;&gt;
&lt;p&gt;Konishi, Sadanori, and Genshiro Kitagawa. 1996. “Generalised Information Criteria in Model Selection.” &lt;em&gt;Biometrika&lt;/em&gt; 83 (4). Oxford University Press: 875–90.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-murata1994network&#34;&gt;
&lt;p&gt;Murata, Noboru, Shuji Yoshizawa, and Shun-ichi Amari. 1994. “Network Information Criterion-Determining the Number of Hidden Units for an Artificial Neural Network Model.” &lt;em&gt;IEEE Transactions on Neural Networks&lt;/em&gt; 5 (6). IEEE: 865–72.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-shimodaira2018information&#34;&gt;
&lt;p&gt;Shimodaira, Hidetoshi, and Haruyoshi Maeda. 2018. “An Information Criterion for Model Selection with Missing Data via Complete-Data Divergence.” &lt;em&gt;Annals of the Institute of Statistical Mathematics&lt;/em&gt; 70 (2). Springer: 421–38.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-takeuchi1976distribution&#34;&gt;
&lt;p&gt;Takeuchi, Kei. 1976. “Distribution of Information Statistics and Validity Criteria of Models.” &lt;em&gt;Mathematical Science&lt;/em&gt; 153: 12–18.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>/slides/example-slides/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/slides/example-slides/</guid>
      <description>

&lt;h1 id=&#34;welcome-to-slides&#34;&gt;Welcome to Slides&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;

&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Code block:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;

&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;

&lt;p&gt;Block math:&lt;/p&gt;

&lt;p&gt;$$
f\left( x \right) = \;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;

&lt;p&gt;Make content appear incrementally&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;

&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;

&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;

&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;


&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;


&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;

&lt;p&gt;Customize the slide style and background&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;

&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://discourse.gohugo.io&#34; target=&#34;_blank&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/&#34; target=&#34;_blank&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
