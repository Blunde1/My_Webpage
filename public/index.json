[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536444000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536444000,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":["Berent Å. S. Lunde","Tore S. Kleppe","Hans J. Skaug"],"categories":null,"content":"","date":1542236400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542236400,"objectID":"6ba90da26fe27518c8fbf92bdf265811","permalink":"/publication/spi/","publishdate":"2018-11-15T00:00:00+01:00","relpermalink":"/publication/spi/","section":"publication","summary":"For certain types of statistical models, the characteristic function (Fourier transform) is available in closed form, whereas the probability density function has an intractable form, typically as an infinite sum of probability weighted densities. Important such examples include solutions of stochastic differential equations with jumps, the Tweedie model, and Poisson mixture models. We propose a novel and general numerical method for retrieving the probability density function from the characteristic function. Unlike methods based on direct application of quadrature to the inverse Fourier transform, the proposed method allows accurate evaluation of the log-probability density function arbitrarily far out in the tail. Moreover, unlike saddlepoint approximations, the proposed methodology can be made arbitrarily accurate. Owing to these properties, the proposed method is computationally stable and very accurate under log-likelihood optimisation. The method is illustrated for a normal variance-mean mixture, and in an application of maximum likelihood estimation to a jump diffusion model for financial data.","tags":["Computational statistics"],"title":"Saddlepoint adjusted inversion of characteristic functions","type":"publication"},{"authors":null,"categories":["R"],"content":"\rFour years ago, back in 2014, a friend from HKUST, that had started working in the finance industry, asked me about the VIX. He could not figure out why the CBOE volatility index, popularly called the fear index, was calculated the way it was (see this white paper) – specifically he was curious about the reason for the term \\(1/K^2\\). Pursuing the quest of understanding the VIX, I remember it felt like very few people truly understood what was going on, and that the ones that did had no interest in sharing their knowledge.\nAfter lots of reading, I was able to connect the dots, summarized here in two parts. The key insights of the first part was found from digging in the references to this wikipedia page; see the appendices in the articles here and here. For the second part I am sad to say I do not remember the references (but I will keep digging).\nI believe this is part of the theory that makes financial mathematics beautiful. My reasons for posting this is due to this inherent beauty, and that I do not want to forget certain insights. Moreover, I hope this can serve some educational purpose for aspiring quants.\nVariance Swap\rBefore we start the calculations it should be noted that, at maturity, the payoff of a variance swap is \\[\r\\begin{align*}\r10000N(\\Sigma^2-K_{var}),\r\\end{align*}\r\\] where\n\r\\(N\\) = notational amount quoted in $ per volatility point squared (hence the factor of \\(100^2=10000\\)).\r\\(\\Sigma^2\\) = realized variance in the underlying asset during the life of the swap.\r\\(K_{var}\\) = strike on variance.\r\rThe underlying assumption is that the S\u0026amp;P500, \\(S_t\\), follows a geometric brownian motion\n\\[\r\\begin{align*}\rdS_t\u0026amp;=\\mu_tS_tdt+\\sigma_tS_tdB_t.\r\\end{align*}\r\\]\nThis can be solved using everyones favourite lemma, Itô’s lemma: Use the substitution \\(Y_t=\\log (S_t)\\) and apply It^o’s Lemma to the function \\(f(t,x):=\\log (x)\\). The partial derivatives are\n\\[\r\\begin{align}\r\\frac{\\partial f}{\\partial t}(t,X_t)=0,\\;\r\\frac{\\partial f}{\\partial x}(t,X_t)=\\frac{1}{S_t},\\;\r\\frac{\\partial^2 f}{\\partial^2 x}(t,X_t)=-\\frac{1}{S_t^2}. \\end{align}\r\\] Thus, we obtain \\[\r\\begin{align*}\rdY_t\u0026amp;=\\left(\\frac{\\mu_tS_t}{S_t}-\\frac{\\sigma_t^2S_t^2}{2S_t^2}\\right)dt+\r\\frac{\\sigma_tS_t}{S_t}dB_t \\text{ by Itô\u0026#39;s Lemma} \\\\\r\u0026amp;=\\left(\\mu_t-\\frac{\\sigma_t^2}{2}\\right)dt+\\sigma_tdB_t.\r\\end{align*}\r\\]\nDividing the equation for \\(dS_t\\) with \\(S_t\\) and subtracting this last equation we have:\n\\[\r\\begin{align*}\r\\frac{dS_t}{S_t}-d(\\log(S_t))\u0026amp;=\\mu_tdt+\\sigma_tdB_t-\\left(\\left(\\mu_t-\\frac{\\sigma_t^2}{2}\\right)dt+\\sigma_tdB_t\\right)\\\\\r\u0026amp;=\\frac{\\sigma_t^2}{2}dt.\r\\end{align*}\r\\] Hence, \\[\r\\begin{align*}\rVariance = \\frac{1}{T}\\int_{0}^{T}\\sigma_t^2dt=\\frac{2}{T}\\left(\\int_{0}^{T}\\frac{dS_t}{S_t}-\\log(\\frac{S_T}{S_0})\\right).\r\\end{align*}\r\\]\nNotice that \\[\r\\begin{align*}\r\\log\\left(\\frac{S_T}{S_0}\\right)=\\log\\left(\\frac{S_T}{S^*}\\right)+\\log\\left(\\frac{S^*}{S_0}\\right).\r\\end{align*}\r\\]\nWe now make use of the dirac delta function (\\(\\delta(x)=\\infty\\) if \\(x=0\\), \\(0\\) otherwise) to write on \\(-\\log\\left(\\frac{S_T}{S_*}\\right)\\) and then integrate by parts twice (this is where the \\(1/K^2\\) comes from):\n\\[\r\\begin{align*}\r-\\log\\left(\\frac{S_T}{S_*}\\right)\u0026amp;=\\log (S^*)-\\log (S_T)\\\\\r\u0026amp;=\\log (S^*)-\\int_{o}^{\\infty}log(K)\\delta(S_T-K)dK\\text{ by propertis of the dirac delta function}\\\\\r\u0026amp;=\\log (S^*)-\\int_{o}^{S^*}log(K)\\delta(S_T-K)dK-\\int_{S^*}^{\\infty}log(K)\\delta(S_T-K)dK \\text{ splitting the integral}\\\\\r\u0026amp;=\\log (S^*)-\\left[\\log(K)1_{(S_T\u0026lt;K)}\\right]_0^{S^*}+\\int_{0}^{S^*}\\frac{1}{K}1_{(S_T\u0026lt;K)}dK\\\\\r\u0026amp;-\\left[\\log(K)1_{(S_T\\geq K)}\\right]_{S^*}^{\\infty}+\\int_{S^*}^{\\infty}\\frac{1}{K}1_{(S_T\\geq K)}dK\\text{ by integration by parts}\\\\\r\u0026amp;=\\log (S^*)- \\log(S^*)1_{(S_T\u0026lt;K)}+\\left.\\frac{1}{K}[K-S_T]^+\\right|_0^{S^*}+\\int_{0}^{S^*}\\frac{1}{K^2}[K-S_T]^+dK\\\\\r\u0026amp;-\\log(S^*)1_{(S_T\\geq K)}+\\left.\\frac{1}{K}[S_T-K]^+\\right|_{S^*}^\\infty+\\int_{S^*}^{\\infty}\\frac{1}{K^2}[S_T-K]^+dK\\\\\r\u0026amp;=\\frac{1}{S^*}\\left(\\left[S^*-S_T\\right]^+-\\left[S_T-S^*\\right]^+\\right)\r+\\int_{0}^{S^*}\\frac{1}{K^2}[K-S_T]^+dK+\\int_{S^*}^{\\infty}\\frac{1}{K^2}[S_T-K]^+dK\\\\\r\u0026amp;=\\frac{1}{S^*}\\left(S^*-S_T\\right)\r+\\int_{0}^{S^*}\\frac{1}{K^2}[K-S_T]^+dK+\\int_{S^*}^{\\infty}\\frac{1}{K^2}[S_T-K]^+dK.\r\\end{align*}\r\\]\nThis was kinda tedious, but this calculation is the reason for \\(1/K^2\\) part in the final equation.\nWe now have the final equation for the variance:\n\\[\r\\begin{align*}\rVariance=\\frac{2}{T}\\left(\\int_{0}^{T}\\frac{dS_t}{S_t}-\\log(\\frac{S^*}{S_0})\r+\\frac{1}{S^*}\\left(S^*-S_T\\right)\r+\\int_{0}^{S^*}\\frac{1}{K^2}[K-S_T]^+dK+\\int_{S^*}^{\\infty}\\frac{1}{K^2}[S_T-K]^+dK\r\\right).\r\\end{align*}\r\\]\nSay \\(Variance = \\Sigma^2\\) (realized variance in the underlying asset during the life of the swap). To avoid arbitrage in the payoff equation (equation located in the start) we have to set: \\[\r\\begin{align*}\rE[\\Sigma^2]=K_{var},\r\\end{align*}\r\\] the expectation of the realized variance is equal to the strike on the variance.\nFor the expecattions, not that \\[\r\\begin{align*}\rE\\left[\\int_{0}^{T}\\frac{dS_t}{S_t}\\right]\u0026amp;=E\\left[\\int_{0}^{T}\\mu_tdt+\\int_{0}^{T}\\sigma_tdB_t\\right]=\\mu T \\text{ assuming }\\mu\\text{ is constant}\\\\\rE\\left[\\log(\\frac{S^*}{S_0})\\right]\u0026amp;=\\log(\\frac{S^*}{S_0})\\text{ they\u0026#39;re just constants}\\\\\rE\\left[S_T\\right]\u0026amp;=S_0e^{\\mu T}.\r\\end{align*}\r\\]\nAlso observe that the price of a put and a call option both with strike \\(K\\) is \\[\r\\begin{align*}\rP(K)\u0026amp;=e^{-eT}E\\left[\\left(K-S_T\\right)^+\\right]\\\\\rC(K)\u0026amp;=e^{-eT}E\\left[\\left(S_T-K\\right)^+\\right].\r\\end{align*}\r\\]\nTaking expectations of the equation for the realized variance we obtain\n\\[\r\\begin{align*}\rK_{var}=E[Variance]=\\frac{2}{T}\\left(\\mu T-\\left(\\frac{S_O}{S^*}e^{\\mu T}-1\\right)\r-\\log\\left(\\frac{S^*}{S_0}\\right)\r+e^{\\mu T}\r\\int_{0}^{S^*}\\frac{P(K)}{K^2}dK+e^{\\mu T}\\int_{S^*}^{\\infty}\\frac{C(K)}{K^2}dK\r\\right).\r\\end{align*}\r\\] Another way of writing this is: \\[\r\\begin{align*}\rK_{var}\u0026amp;=\\frac{2}{T}\\left(\\log\\left(\\frac{F_o}{S^*}\\right)-\\left(\\frac{F_0}{S^*}-1\\right)+e^{\\mu T}\r\\int_{0}^{S^*}\\frac{P(K)}{K^2}dK+e^{\\mu T}\\int_{S^*}^{\\infty}\\frac{C(K)}{K^2}dK\r\\right).\r\\end{align*}\r\\] Now, making the smart choice of setting \\(S^*=F_0=S_0e^{\\mu T}\\) a lot cancel out and we have the final equation\n\\[\r\\begin{align*}\rK_{var}=\\frac{2e^{\\mu T}}{T}\\left[\\int_{0}^{F_0}\\frac{P(K)}{K^2}dK+\\int_{F_0}^{\\infty}\\frac{C(K)}{K^2}dK\\right].\r\\end{align*}\r\\]\n\rRelationship between variance swap and the VIX\rWe seek a discretized version (the VIX) of our fair price variance swap formula. Assume that the price of options with strike prices \\(K_i,\\,(i=1:n)\\) are known and that \\[\r\\begin{align*}\rK_1\u0026lt;K_2\u0026lt;...\u0026lt;K_n.\r\\end{align*}\r\\] Furthermore, choose \\(S^*\\) equal to the first strike price below \\(F_0\\), define the function\n\\[\r\\begin{align*}\rQ(K_i)\u0026amp;=P(K_i)1_{(K_i\\leq S^*)}+C(K_i)1_{(S^*\u0026lt;K_i)},\r\\end{align*} \\]\nand then approximate the integrals as following\n\\[\r\\begin{align*}\re^{\\mu T}\r\\int_{0}^{S^*}\\frac{P(K)}{K^2}dK+e^{\\mu T}\\int_{S^*}^{\\infty}\\frac{C(K)}{K^2}dK=e^{\\mu T}\\sum_{i=1}^{n}\\frac{\\Delta K_i}{K_i^2}Q(K_i),\r\\end{align*}\r\\]\nwhere \\[\r\\begin{align*}\r\\Delta K_i\u0026amp;=\\frac{K_{i+1}-K_{i}}{2},\\,\\,i=2:n-1\\\\\r\\Delta K_1\u0026amp;=K_2-K_1\\\\\r\\Delta K_n\u0026amp;=K_n-K_{n-1}.\r\\end{align*}\r\\]\nSet this aproximation inside the equation for fair variance swap with and we obtain \\[\r\\begin{align*}\rK_{var}\\approx\\frac{2}{T}\\left(\\log\\left(\\frac{F_o}{S^*}\\right)-\\left(\\frac{F_0}{S^*}-1\\right)+e^{\\mu T}\\sum_{i=1}^{n}\\frac{\\Delta K_i}{K_i^2}Q(K_i)\\right).\r\\end{align*}\r\\]\nThe maclaurin polynomial for \\(\\log(F_0/S^*)\\) is \\[\r\\begin{align*}\r\\log\\left(\\frac{F_0}{S^*}\\right)\u0026amp;=\\left(\\frac{F_0}{S^*}-1\\right)-\\frac{1}{2}\\left(\\frac{F_0}{S^*}-1\\right)^2+O\\left(\\left(\\frac{F_0}{S^*}-1\\right)^2\\right).\r\\end{align*}\r\\]\nThus, \\[\r\\begin{align*}\r\\log\\left(\\frac{F_o}{S^*}\\right)-\\left(\\frac{F_0}{S^*}-1\\right)\r=-\\frac{1}{2}\\left(\\frac{F_0}{S^*}-1\\right)^2+O\\left(\\left(\\frac{F_0}{S^*}-1\\right)^2\\right),\r\\end{align*}\r\\] and we obtain our final approximation\n\\[\r\\begin{align*}\rK_{var}\\approx\\frac{2}{T}e^{\\mu T}\\sum_{i=1}^{n}\\frac{\\Delta K_i}{K_i^2}Q(K_i)-\\frac{1}{2}\\left(\\frac{F_0}{S^*}-1\\right)^2,\r\\end{align*}\r\\] which we recognize as the fomula for the VIX.\nRemember that the underlying “stock” here is the S\u0026amp;P500 index! I am note sure if there are call and put options that has the S\u0026amp;P as underlying, if there isn’t, you should use underlying stocks that is highly correlated (beta) with the S\u0026amp;P and calculate the VIX for all of them and maybe take the average.\n\r","date":1541980800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541980800,"objectID":"2e17fd9f47b557f664fdceaf494e512f","permalink":"/post/the-fear-index-vix-and-variance-swaps/","publishdate":"2018-11-12T00:00:00Z","relpermalink":"/post/the-fear-index-vix-and-variance-swaps/","section":"post","summary":"Four years ago, back in 2014, a friend from HKUST, that had started working in the finance industry, asked me about the VIX. He could not figure out why the CBOE volatility index, popularly called the fear index, was calculated the way it was (see this white paper) – specifically he was curious about the reason for the term \\(1/K^2\\). Pursuing the quest of understanding the VIX, I remember it felt like very few people truly understood what was going on, and that the ones that did had no interest in sharing their knowledge.","tags":["R Markdown"],"title":"The fear index, VIX and variance swaps","type":"post"},{"authors":null,"categories":null,"content":"","date":1541977200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541977200,"objectID":"78f8bec805d0b2977292bf35cf724c16","permalink":"/project/nordic-quantlab/nql_project/","publishdate":"2018-11-12T00:00:00+01:00","relpermalink":"/project/nordic-quantlab/nql_project/","section":"project","summary":"Lean AI at your fingertips..","tags":["Boosting","Machine-Learning"],"title":"Nordic QuantLab","type":"project"},{"authors":null,"categories":null,"content":"","date":1541977200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541977200,"objectID":"c5e077d0131195f3d201a081b5a92219","permalink":"/project/exactspa/exactspa/","publishdate":"2018-11-12T00:00:00+01:00","relpermalink":"/project/exactspa/exactspa/","section":"project","summary":"Saddlepoint adjusted inversion of characteristic functions.","tags":["Computational statistics"],"title":"SPI","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["Berent Å. S. Lunde","Tore S. Kleppe","Hans J. Skaug"],"categories":null,"content":"A repository for the presentation can be found at here.\n","date":1529445600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529445600,"objectID":"5ff227098e3a3a2d020f7958a3bc93e1","permalink":"/talk/ecosta-2018/","publishdate":"2018-06-20T00:00:00+02:00","relpermalink":"/talk/ecosta-2018/","section":"talk","summary":"For certain types of statistical models, the characteristic function (Fourier transform) is available in closed form, whereas the probability density function has an intractable form, typically as an infinite sum of probability weighted densities. Important such examples include solutions of stochastic differential equations with jumps, the Tweedie model, and Poisson mixture models. We propose a novel and general numerical method for retrieving the probability density function from the characteristic function. Unlike methods based on direct application of quadrature to the inverse Fourier transform, the proposed method allows accurate evaluation of the log-probability density function arbitrarily far out in the tail. Moreover, unlike saddlepoint approximations, the proposed methodology can be made arbitrarily accurate. Owing to these properties, the proposed method is computationally stable and very accurate under log-likelihood optimisation. The method is illustrated for a normal variance-mean mixture, and in an application of maximum likelihood estimation to a jump diffusion model for financial data.","tags":["Computational statistics"],"title":"Saddlepoint adjusted inversion of characteristic functions","type":"talk"},{"authors":null,"categories":null,"content":"","date":1526076000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526076000,"objectID":"5ef05da144c4640bed0108b58f818f51","permalink":"/project/aqualens/aqualens/","publishdate":"2018-05-12T00:00:00+02:00","relpermalink":"/project/aqualens/aqualens/","section":"project","summary":"Enhancing the value of all collected data.","tags":["Data Science","Aquahack","Hackathon"],"title":"AquaLens","type":"project"},{"authors":["Berent Lunde"],"categories":null,"content":"A repository for the presentation can be found at here.\n","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"e48a2a9c3d73d6275e7cd19053bce5cc","permalink":"/talk/bergen-ml-meetup-2018-10-31/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/talk/bergen-ml-meetup-2018-10-31/","section":"talk","summary":"At the next Bergen Machine Learning meetup I will lecture on some of my work with gradient boosting algorithms: by coupling information theory, the frequency domain and tree-boosting, the algorithm can adaptively learn the optimal structure of individual trees, and how many trees that should be added; regularization is redundant. This is nice as there are no worries of overfitting, the computational cost is drastically reduced, and it facilitates the democratization of machine learning.","tags":["Boosting","Machine-Learning"],"title":"Information efficient gradient tree boosting","type":"talk"},{"authors":null,"categories":null,"content":"\r Outline\r\rBackground\r\rSupervised learning\rGradient boosting\r\rInformation efficient GTB\r\rFrequency domain\rJapanese tricks\rBottoms up\r\rExperimental illustrations\r\rBias\rConvergence\rComparisons\r\rRecap and possibilities\r\r\rBackground\rSupervised learning\r\r\rSupervised learning\r\r\rMapping \\(f:A \\rightarrow B\\) using a finite dataset.\rWhat is a good model?\r\rA model that generalises well to unseen data.\r\r\rGeneralises: average over new data\r\r\rwell: minimises Loss\r\r\\(\\tilde{f} = \\arg\\min_f {\\mathbb{E}}_{\\hat{w}}{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[{\\mathcal{L}}\\left({y}, f({\\mathbf{x}}; \\hat{w})\\right)\\right]\\)\r\r\r\rGradient boosting\rAlgorithm:\nInitialize model with a constant value: \\(F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^n {\\mathcal{L}}(y_i, \\gamma).\\)\rFor \\(m = 1\\) to \\(M\\):\rCompute so-called ‘’pseudo-residuals’’: \\(r_{im} = -\\left[\\frac{\\partial {\\mathcal{L}}(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)} \\quad \\mbox{for } i=1,\\ldots,n.\\)\rFit a base learner (e.g. tree) \\(h_m(x)\\) to pseudo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^n\\).\rCompute multiplier \\(\\gamma_m\\) by solving the following one-dimensional optimization problem: \\(\\gamma_m = \\underset{\\gamma}{\\operatorname{arg\\,min}} \\sum_{i=1}^n {\\mathcal{L}}\\left(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\right)\\).\rUpdate the model: \\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x).\\)\r\rOutput \\(F_M(x)\\).\r\r\rGradient tree boosting\rAt iteration \\(\\small m+1\\):\n\rComputes \\(\\small g_i=-r_i\\) and \\(\\small h_i=\\left[\\frac{\\partial^2 {\\mathcal{L}}(y_i, F(x_i))}{\\partial F(x_i)^2}\\right]_{F(x)=F_{m-1}(x)}\\quad \\mbox{for } i=1,\\ldots,n.\\)\rApproximates \\({\\mathcal{L}}\\) by a second order approximation about \\(F_{m}\\): \\[{\\small\r{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[ {\\mathcal{L}}({y}, (f_m+f)({\\mathbf{x}}) \\right] \\approx\r\\frac{1}{n}\\sum_{i=1}^{n} {\\mathcal{L}}(y_i,f_m({\\mathbf{x}}_i))+g_i f({\\mathbf{x}}_i)\r+\\frac{1}{2}h_if({\\mathbf{x}}_i)^2\r}\\]\rSolves the quadratic problem exactly: \\(\\small \\hat{w} = -\\frac{\\sum g_i}{\\sum h_i}\\) and \\(\\small {\\mathcal{L}}(y,\\hat{w}) = -\\frac{\\left(\\sum g_i\\right)^2}{2\\sum h_i}\\)\rSplits nodes to maximize loss reduction:\r\r\\({\\small \\frac{1}{2}\\left[\\frac{\\left(\\sum_{i\\in I_L(j,s)}g_i\\right)^2}{\\sum_{i\\in I_L(j,s)}h_i} + \\frac{\\left(\\sum_{i\\in I_R(j,s)}g_i\\right)^2}{\\sum_{i\\in I_R(j,s)}h_i} -\\frac{\\left(\\sum_{i\\in I_k}g_i\\right)^2}{\\sum_{i\\in I_k}h_i}\\right] }\\)\r\r\r\rThe problem\r\rWrong objective:\n\\({\\mathcal{L}}(y,\\hat{w}) = -\\frac{\\left(\\sum g_i\\right)^2}{2\\sum h_i}\\) is not an unbiased estimator of \\({\\mathbb{E}}_{\\hat{w}}{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[{\\mathcal{L}}\\left({y}, f({\\mathbf{x}}; \\hat{w})\\right)\\right]\\)\rRegularization: \\({\\mathcal{L}}+ \\Omega(w)\\)\rXGBoost and LightGBM each have \u0026gt; 10 hyperparameters that should be configured.\rHigh dimensional optimisation is difficult.\rTime consuming.\rRequires human expertiese.\rCross validation is information inefficient.\r\rVariable results.\rRisky in terms of over and underfitting.\rKaggle mantra: trust your local validation…\r\r\r\r\rInformation efficient gradient tree boosting\rRetrieval of the estimator distribution\rFor model selection purposes:\n\rSeek the distribution of \\(\\hat{w} = -\\frac{\\sum g_i}{\\sum h_i}\\).\r\rHowever:\nDistribution is unknown unless prior information about the data-generating process is known.\rAsymptotic theory assumptions does not hold in tree-leaves.\rBootstrapping to estimate the empirical/ resample distribution is extremely costly.\r\r\rBut we know the probability of resampling one observation event: \\(\\frac{1}{n}\\)\rWe also know the one-observation resample events characteristic function (Fourier transform): \\[\\small {\\varphi^*}_{g,h}(u,v) = \\frac{1}{n} \\sum_{j=1}^n e^{iug_j + ivh_j}\\]\r\r\rThe frequency domain\r\rThe frequency domain is a fantastic place to combine randomness\r\rThe characteristic function takes a frequency, \\(s\\), as argument: \\[\\small\r\\varphi_X(s) = {\\mathbb{E}}\\left[e^{isx}\\right] = \\int e^{isx}p_X(x)~dx\\]\n\r\\(s\\) is the angular frequency \\(s=2\\pi \\tau\\), \\(\\tau\\) is the number of rotations per unit of time.\rThe average of \\(X\\) wrapped around the unit circle with angular frequency \\(s\\).\r\\(\\varphi_X(s)\\) is called the Fourier transform of \\(p_X(x)\\).\r\r\rThe frequency domain\rThe characteristic function takes a frequency, \\(s\\), as argument: \\[ \\small\r\\varphi_X(s) = {\\mathbb{E}}\\left[e^{isx}\\right] = \\int e^{isx}p_X(x)~dx\\]\n\rThe average of \\(X\\) wrapped around the unit circle with angular frequency \\(s\\). \r\r\rProperty of the characteristic function\rGeneral property, if \\(\\small X_1\\) and \\(\\small X_2\\) are independent, then \\[\\small\r\\varphi_{\\left(X_1+X_2\\right)}(s) = \\varphi_{X_1}(s) \\times \\varphi_{X_2}(s).\\] Thus \\[\r\\begin{align}\\label{eq empirical cf}\r\\varphi_{\\sum g,\\sum h}^*(u,v) = \\left[ \\varphi_{g, h}^*\\left(u,v\\right) \\right]^n.\r\\end{align}\\]\nWe can obtain the density and distribution functions: \\[\\small\r\\begin{align*}\rp_X(x) \u0026amp;= \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty\r\\varphi_X(s)e^{-isx} ds.\r\\end{align*}\\]\n\rBack to the actual problem\rWhat we have: training error – the error on the data we trained on \\[\\small\r{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[ {\\mathcal{L}}({y},F({\\mathbf{x}};\\hat{w}({\\mathbf{x}},{y})))\\right].\\]\nWhat we need: test error – the error on previously unseen data \\[\\small\r{\\mathbb{E}}_{\\hat{w}}{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[{\\mathcal{L}}\\left({y}, F({\\mathbf{x}}; \\hat{w})\\right)\\right].\\]\nHow to get it?\nThis is a known problem!\n\r\rAkaike (1974): log-likelihood, asymptotics, true model is in the modelspace.\rTakeuchi (1976): generalises AIC – true model not necessarily in the modelspace.\rMurata, Yoshizawa, and Amari (1994): generalises TIC – differentiable loss function\rKonishi and Kitagawa (1996)\rShimodaira and Maeda (2018): generalises TIC to missing data\r\r\r\rAn information criterion for GTB\r\rFigure 1: Geometric view of Japanese tricks\r\r\rAvoids asymptotics by utilizing the frequency domain.\rExploit the iterative structure of gradient boosting and the locally constant structure of trees.\rAvoids high-dimensional matrix inversion (for possibly non-positive definite matrices). \\[\\small\r\\begin{align}\\label{eq criterion exact}\r{\\mathbb{E}}_{f_m}{\\mathbb{E}}_{{y}} \\left[ \\hat{{\\mathcal{L}}}(y,\\hat{f}_m) \\right]\r=\r{\\mathbb{E}}_{{y}} \\left[ \\hat{{\\mathcal{L}}}(y,\\hat{f}_m) \\right]\r\u0026amp;+ \\frac{1}{2} \\sum_{k=1}^{T}{\\mathbb{E}}_{{y}} \\left[ h| q({\\mathbf{x}})=k \\right ]{\\mathrm{Var}}[w_k]\\notag\\\\\r\u0026amp; + \\frac{1}{2} \\sum_{k=1}^{T}{\\mathbb{E}}_{{y}} \\left[ \\left\\{ h| q({\\mathbf{x}})=k \\right\\} (w_0 - \\hat{w}_k)^2 \\right].\r\\end{align}\\]\r\r\rBottoms up: branch pruning\r\r\rExperimental illustrations\rBias\r\rFocus on the individual bias in a node.\rRepeated simulation experiment to obtain the distribution of bias estimators.\r\rCross validation is information inefficient.\rAsymptotic criteria, NIC, is biased downward.\rLIC is slightly biased upwards.\r\r\rConvergence\r\rTraining, testing, and estimated test-loss versus number of trees added.\rNumber of leaves in n’th tree.\r\rConvergence in number of trees: The estimted test-loss minimum correspond to the testing loss.\rThe number of leaves decreases as information is learned by the model.\r\r\rComparisons on Titanic data\r\rComparisons versus a regularized model and one trained with a validation dataset (30% of training).\rRepeated random splitting in training and test datasets.\r\rOur information efficient approach is slightly better overall (mean).\rThe standard deviation is the lowest for our approach (consistency and information efficient).\r\r\r\rRecap and possibilities\rTwo different approaches\r\r\rCriteria\rRegularized\rInformation efficient\r\r\r\rSpeed:\r\\(10\\times\\) number of grid points\rOne run\r\rAutomated:\rNo\rYes\r\rUser knowledge:\rSome\rNone\r\rRisks:\rOver and underfitting\rNone\r\rOptimal:\rClose\rYes\r\r\r\r\rNew possibilities\r\rUsers without deep ML knowledge. \rBuild lots of models, very fast.\r\rAndrew Ng: “Anything a typical person can do with one second of thought, we can probably now or soon automate.”\r\rOnline learning.\rInner optimization routines (Gaussian Markov random fields etc.).\n\r\r\rReferences\rAkaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” IEEE Transactions on Automatic Control 19 (6). Ieee: 716–23.\n\rKonishi, Sadanori, and Genshiro Kitagawa. 1996. “Generalised Information Criteria in Model Selection.” Biometrika 83 (4). Oxford University Press: 875–90.\n\rMurata, Noboru, Shuji Yoshizawa, and Shun-ichi Amari. 1994. “Network Information Criterion-Determining the Number of Hidden Units for an Artificial Neural Network Model.” IEEE Transactions on Neural Networks 5 (6). IEEE: 865–72.\n\rShimodaira, Hidetoshi, and Haruyoshi Maeda. 2018. “An Information Criterion for Model Selection with Missing Data via Complete-Data Divergence.” Annals of the Institute of Statistical Mathematics 70 (2). Springer: 421–38.\n\rTakeuchi, Kei. 1976. “Distribution of Information Statistics and Validity Criteria of Models.” Mathematical Science 153: 12–18.\n\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a18291993562c60e75963f77cbb3d602","permalink":"/slides/mlmeetup/mlmeetup_presentation_bl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/mlmeetup/mlmeetup_presentation_bl/","section":"slides","summary":"Outline\r\rBackground\r\rSupervised learning\rGradient boosting\r\rInformation efficient GTB\r\rFrequency domain\rJapanese tricks\rBottoms up\r\rExperimental illustrations\r\rBias\rConvergence\rComparisons\r\rRecap and possibilities\r\r\rBackground\rSupervised learning\r\r\rSupervised learning\r\r\rMapping \\(f:A \\rightarrow B\\) using a finite dataset.\rWhat is a good model?\r\rA model that generalises well to unseen data.\r\r\rGeneralises: average over new data\r\r\rwell: minimises Loss\r\r\\(\\tilde{f} = \\arg\\min_f {\\mathbb{E}}_{\\hat{w}}{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[{\\mathcal{L}}\\left({y}, f({\\mathbf{x}}; \\hat{w})\\right)\\right]\\)\r\r\r\rGradient boosting\rAlgorithm:","tags":null,"title":"Information efficient gradient tree boosting","type":"slides"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]