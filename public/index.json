[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536444000,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536444000,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"","date":1541977200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541977200,"objectID":"78f8bec805d0b2977292bf35cf724c16","permalink":"/project/nordic-quantlab/nql_project/","publishdate":"2018-11-12T00:00:00+01:00","relpermalink":"/project/nordic-quantlab/nql_project/","section":"project","summary":"Lean AI at your fingertips..","tags":["Boosting","Machine-Learning"],"title":"Nordic QuantLab","type":"project"},{"authors":null,"categories":null,"content":"","date":1541977200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541977200,"objectID":"c5e077d0131195f3d201a081b5a92219","permalink":"/project/exactspa/exactspa/","publishdate":"2018-11-12T00:00:00+01:00","relpermalink":"/project/exactspa/exactspa/","section":"project","summary":"Saddlepoint adjusted inversion of characteristic functions.","tags":["Computational statistics"],"title":"SPI","type":"project"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536444000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536444000,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"/tutorial/example/","publishdate":"2018-09-09T00:00:00+02:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":null,"categories":null,"content":"","date":1526076000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526076000,"objectID":"5ef05da144c4640bed0108b58f818f51","permalink":"/project/aqualens/aqualens/","publishdate":"2018-05-12T00:00:00+02:00","relpermalink":"/project/aqualens/aqualens/","section":"project","summary":"Enhancing the value of all collected data.","tags":["Data Science","Aquahack","Hackathon"],"title":"AquaLens","type":"project"},{"authors":["Berent Lunde"],"categories":null,"content":"A repository for the presentation can be found at here.\n","date":1483225200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483225200,"objectID":"e48a2a9c3d73d6275e7cd19053bce5cc","permalink":"/talk/bergen-ml-meetup-2018-10-31/","publishdate":"2017-01-01T00:00:00+01:00","relpermalink":"/talk/bergen-ml-meetup-2018-10-31/","section":"talk","summary":"At the next Bergen Machine Learning meetup I will lecture on some of my work with gradient boosting algorithms: by coupling information theory, the frequency domain and tree-boosting, the algorithm can adaptively learn the optimal structure of individual trees, and how many trees that should be added; regularization is redundant. This is nice as there are no worries of overfitting, the computational cost is drastically reduced, and it facilitates the democratization of machine learning.","tags":["Boosting","Machine-Learning"],"title":"Information efficient gradient tree boosting","type":"talk"},{"authors":[],"categories":null,"content":" Academic makes it easy to create a beautiful website for free using Markdown. Customize anything on your site with widgets, themes, and language packs.\nFollow our easy step by step guide to learn how to build your own free website with Academic. Check out the personal demo or the business demo of what you\u0026rsquo;ll get in less than 10 minutes.\n View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of Academic:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an Academic sticker Wear the T-shirt   \nKey features:\n Easily manage various content including homepage, blog posts, publications, talks, and projects Extensible via color themes and widgets/plugins Write in Markdown for easy formatting and code highlighting, with LaTeX for mathematical expressions Social/academic network linking, Google Analytics, and Disqus comments Responsive and mobile friendly Simple and refreshing one page design Multilingual and easy to customize  Color Themes Academic is available in different color themes and font themes.\n         Ecosystem  Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Quick install using your web browser  Install Academic with Netlify  Netlify will provide you with a customizable URL to access your new site  On GitHub, go to your newly created academic-kickstart repository and edit config.toml to personalize your site. Shortly after saving the file, your site will automatically update Read the Quick Start Guide to learn how to add Markdown content. For inspiration, refer to the Markdown content which powers the Demo  Install with Git Prerequisites:\n Download and install Git Download and install Hugo   Fork the Academic Kickstart repository and clone your fork with Git:\ngit clone https://github.com/sourcethemes/academic-kickstart.git My_Website  Note that if you forked Academic Kickstart, the above command should be edited to clone your fork, i.e. replace sourcethemes with your GitHub username.\n Initialize the theme:\ncd My_Website git submodule update --init --recursive   Install with ZIP  Download and extract Academic Kickstart Download and extract the Academic theme to the themes/academic/ folder from the above step  Install with RStudio View the guide to installing Academic with RStudio\nQuick start  If you installed on your computer, view your new website by running the following command:\nhugo server  Now visit localhost:1313 and your new Academic powered website will appear. Otherwise, if using Netlify, they will provide you with your URL.\n Read the Quick Start Guide to learn how to add Markdown content, customize your site, and deploy it. For inspiration, refer to the Markdown content which powers the Demo\n Build your site by running the hugo command. Then host it for free using Github Pages or Netlify (refer to the first installation method). Alternatively, copy the generated public/ directory (by FTP, Rsync, etc.) to your production web server (such as a university\u0026rsquo;s hosting service).\n  Updating Feel free to star the project on Github to help keep track of updates and check out the release notes prior to updating your site.\nBefore updating the framework, it is recommended to make a backup of your entire website directory (or at least your themes/academic directory) and record your current version number.\nBy default, Academic is installed as a Git submodule which can be updated by running the following command:\ngit submodule update --remote --merge  Check out the update guide for full instructions and alternative methods.\nFeedback \u0026amp; Contributing Please use the issue tracker to let me know about any bugs or feature requests, or alternatively make a pull request.\nFor support, head over to the Hugo discussion forum.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461103200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515798000,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2016-04-20T00:00:00+02:00","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website or blog in under 10 minutes.","tags":["Academic"],"title":"Academic: the website designer for Hugo","type":"post"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441058400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441058400,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"/publication/person-re-id/","publishdate":"2015-09-01T00:00:00+02:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":null,"categories":["R"],"content":"\rR Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00\rfit \u0026lt;- lm(dist ~ speed, data = cars)\rfit\r## ## Call:\r## lm(formula = dist ~ speed, data = cars)\r## ## Coefficients:\r## (Intercept) speed ## -17.579 3.932\r\rIncluding Plots\rYou can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1))\rpie(\rc(280, 60, 20),\rc(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;),\rcol = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;),\rinit.angle = -50, border = NA\r)\r\rFigure 1: A fancy pie chart.\r\r\r","date":1437703994,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1437703994,"objectID":"10065deaa3098b0da91b78b48d0efc71","permalink":"/post/2015-07-23-r-rmarkdown/","publishdate":"2015-07-23T21:13:14-05:00","relpermalink":"/post/2015-07-23-r-rmarkdown/","section":"post","summary":"R Markdown\rThis is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars)\r## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372629600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372629600,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"/publication/clothing-search/","publishdate":"2013-07-01T00:00:00+02:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":"\r Outline\r\rBackground\r\rSupervised learning\rGradient boosting\r\rInformation efficient GTB\r\rFrequency domain\rJapanese tricks\rBottoms up\r\rExperimental illustrations\r\rBias\rConvergence\rComparisons\r\rRecap and possibilities\r\r\rBackground\rSupervised learning\r\r\rSupervised learning\r\r\rMapping \\(f:A \\rightarrow B\\) using a finite dataset.\rWhat is a good model?\r\rA model that generalises well to unseen data.\r\r\rGeneralises: average over new data\r\r\rwell: minimises Loss\r\r\\(\\tilde{f} = \\arg\\min_f {\\mathbb{E}}_{\\hat{w}}{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[{\\mathcal{L}}\\left({y}, f({\\mathbf{x}}; \\hat{w})\\right)\\right]\\)\r\r\r\rGradient boosting\rAlgorithm:\nInitialize model with a constant value: \\(F_0(x) = \\underset{\\gamma}{\\arg\\min} \\sum_{i=1}^n {\\mathcal{L}}(y_i, \\gamma).\\)\rFor \\(m = 1\\) to \\(M\\):\rCompute so-called ‘’pseudo-residuals’’: \\(r_{im} = -\\left[\\frac{\\partial {\\mathcal{L}}(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(x)=F_{m-1}(x)} \\quad \\mbox{for } i=1,\\ldots,n.\\)\rFit a base learner (e.g. tree) \\(h_m(x)\\) to pseudo-residuals, i.e. train it using the training set \\(\\{(x_i, r_{im})\\}_{i=1}^n\\).\rCompute multiplier \\(\\gamma_m\\) by solving the following one-dimensional optimization problem: \\(\\gamma_m = \\underset{\\gamma}{\\operatorname{arg\\,min}} \\sum_{i=1}^n {\\mathcal{L}}\\left(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\right)\\).\rUpdate the model: \\(F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x).\\)\r\rOutput \\(F_M(x)\\).\r\r\rGradient tree boosting\rAt iteration \\(\\small m+1\\):\n\rComputes \\(\\small g_i=-r_i\\) and \\(\\small h_i=\\left[\\frac{\\partial^2 {\\mathcal{L}}(y_i, F(x_i))}{\\partial F(x_i)^2}\\right]_{F(x)=F_{m-1}(x)}\\quad \\mbox{for } i=1,\\ldots,n.\\)\rApproximates \\({\\mathcal{L}}\\) by a second order approximation about \\(F_{m}\\): \\[{\\small\r{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[ {\\mathcal{L}}({y}, (f_m+f)({\\mathbf{x}}) \\right] \\approx\r\\frac{1}{n}\\sum_{i=1}^{n} {\\mathcal{L}}(y_i,f_m({\\mathbf{x}}_i))+g_i f({\\mathbf{x}}_i)\r+\\frac{1}{2}h_if({\\mathbf{x}}_i)^2\r}\\]\rSolves the quadratic problem exactly: \\(\\small \\hat{w} = -\\frac{\\sum g_i}{\\sum h_i}\\) and \\(\\small {\\mathcal{L}}(y,\\hat{w}) = -\\frac{\\left(\\sum g_i\\right)^2}{2\\sum h_i}\\)\rSplits nodes to maximize loss reduction:\r\r\\({\\small \\frac{1}{2}\\left[\\frac{\\left(\\sum_{i\\in I_L(j,s)}g_i\\right)^2}{\\sum_{i\\in I_L(j,s)}h_i} + \\frac{\\left(\\sum_{i\\in I_R(j,s)}g_i\\right)^2}{\\sum_{i\\in I_R(j,s)}h_i} -\\frac{\\left(\\sum_{i\\in I_k}g_i\\right)^2}{\\sum_{i\\in I_k}h_i}\\right] }\\)\r\r\r\rThe problem\r\rWrong objective:\n\\({\\mathcal{L}}(y,\\hat{w}) = -\\frac{\\left(\\sum g_i\\right)^2}{2\\sum h_i}\\) is not an unbiased estimator of \\({\\mathbb{E}}_{\\hat{w}}{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[{\\mathcal{L}}\\left({y}, f({\\mathbf{x}}; \\hat{w})\\right)\\right]\\)\rRegularization: \\({\\mathcal{L}}+ \\Omega(w)\\)\rXGBoost and LightGBM each have \u0026gt; 10 hyperparameters that should be configured.\rHigh dimensional optimisation is difficult.\rTime consuming.\rRequires human expertiese.\rCross validation is information inefficient.\r\rVariable results.\rRisky in terms of over and underfitting.\rKaggle mantra: trust your local validation…\r\r\r\r\rInformation efficient gradient tree boosting\rRetrieval of the estimator distribution\rFor model selection purposes:\n\rSeek the distribution of \\(\\hat{w} = -\\frac{\\sum g_i}{\\sum h_i}\\).\r\rHowever:\nDistribution is unknown unless prior information about the data-generating process is known.\rAsymptotic theory assumptions does not hold in tree-leaves.\rBootstrapping to estimate the empirical/ resample distribution is extremely costly.\r\r\rBut we know the probability of resampling one observation event: \\(\\frac{1}{n}\\)\rWe also know the one-observation resample events characteristic function (Fourier transform): \\[\\small {\\varphi^*}_{g,h}(u,v) = \\frac{1}{n} \\sum_{j=1}^n e^{iug_j + ivh_j}\\]\r\r\rThe frequency domain\r\rThe frequency domain is a fantastic place to combine randomness\r\rThe characteristic function takes a frequency, \\(s\\), as argument: \\[\\small\r\\varphi_X(s) = {\\mathbb{E}}\\left[e^{isx}\\right] = \\int e^{isx}p_X(x)~dx\\]\n\r\\(s\\) is the angular frequency \\(s=2\\pi \\tau\\), \\(\\tau\\) is the number of rotations per unit of time.\rThe average of \\(X\\) wrapped around the unit circle with angular frequency \\(s\\).\r\\(\\varphi_X(s)\\) is called the Fourier transform of \\(p_X(x)\\).\r\r\rThe frequency domain\rThe characteristic function takes a frequency, \\(s\\), as argument: \\[ \\small\r\\varphi_X(s) = {\\mathbb{E}}\\left[e^{isx}\\right] = \\int e^{isx}p_X(x)~dx\\]\n\rThe average of \\(X\\) wrapped around the unit circle with angular frequency \\(s\\). \r\r\rProperty of the characteristic function\rGeneral property, if \\(\\small X_1\\) and \\(\\small X_2\\) are independent, then \\[\\small\r\\varphi_{\\left(X_1+X_2\\right)}(s) = \\varphi_{X_1}(s) \\times \\varphi_{X_2}(s).\\] Thus \\[\r\\begin{align}\\label{eq empirical cf}\r\\varphi_{\\sum g,\\sum h}^*(u,v) = \\left[ \\varphi_{g, h}^*\\left(u,v\\right) \\right]^n.\r\\end{align}\\]\nWe can obtain the density and distribution functions: \\[\\small\r\\begin{align*}\rp_X(x) \u0026amp;= \\frac{1}{2\\pi} \\int_{-\\infty}^\\infty\r\\varphi_X(s)e^{-isx} ds.\r\\end{align*}\\]\n\rBack to the actual problem\rWhat we have: training error – the error on the data we trained on \\[\\small\r{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[ {\\mathcal{L}}({y},F({\\mathbf{x}};\\hat{w}({\\mathbf{x}},{y})))\\right].\\]\nWhat we need: test error – the error on previously unseen data \\[\\small\r{\\mathbb{E}}_{\\hat{w}}{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[{\\mathcal{L}}\\left({y}, F({\\mathbf{x}}; \\hat{w})\\right)\\right].\\]\nHow to get it?\nThis is a known problem!\n\r\rAkaike (1974): log-likelihood, asymptotics, true model is in the modelspace.\rTakeuchi (1976): generalises AIC – true model not necessarily in the modelspace.\rMurata, Yoshizawa, and Amari (1994): generalises TIC – differentiable loss function\rKonishi and Kitagawa (1996)\rShimodaira and Maeda (2018): generalises TIC to missing data\r\r\r\rAn information criterion for GTB\r\rFigure 1: Geometric view of Japanese tricks\r\r\rAvoids asymptotics by utilizing the frequency domain.\rExploit the iterative structure of gradient boosting and the locally constant structure of trees.\rAvoids high-dimensional matrix inversion (for possibly non-positive definite matrices). \\[\\small\r\\begin{align}\\label{eq criterion exact}\r{\\mathbb{E}}_{f_m}{\\mathbb{E}}_{{y}} \\left[ \\hat{{\\mathcal{L}}}(y,\\hat{f}_m) \\right]\r=\r{\\mathbb{E}}_{{y}} \\left[ \\hat{{\\mathcal{L}}}(y,\\hat{f}_m) \\right]\r\u0026amp;+ \\frac{1}{2} \\sum_{k=1}^{T}{\\mathbb{E}}_{{y}} \\left[ h| q({\\mathbf{x}})=k \\right ]{\\mathrm{Var}}[w_k]\\notag\\\\\r\u0026amp; + \\frac{1}{2} \\sum_{k=1}^{T}{\\mathbb{E}}_{{y}} \\left[ \\left\\{ h| q({\\mathbf{x}})=k \\right\\} (w_0 - \\hat{w}_k)^2 \\right].\r\\end{align}\\]\r\r\rBottoms up: branch pruning\r\r\rExperimental illustrations\rBias\r\rFocus on the individual bias in a node.\rRepeated simulation experiment to obtain the distribution of bias estimators.\r\rCross validation is information inefficient.\rAsymptotic criteria, NIC, is biased downward.\rLIC is slightly biased upwards.\r\r\rConvergence\r\rTraining, testing, and estimated test-loss versus number of trees added.\rNumber of leaves in n’th tree.\r\rConvergence in number of trees: The estimted test-loss minimum correspond to the testing loss.\rThe number of leaves decreases as information is learned by the model.\r\r\rComparisons on Titanic data\r\rComparisons versus a regularized model and one trained with a validation dataset (30% of training).\rRepeated random splitting in training and test datasets.\r\rOur information efficient approach is slightly better overall (mean).\rThe standard deviation is the lowest for our approach (consistency and information efficient).\r\r\r\rRecap and possibilities\rTwo different approaches\r\r\rCriteria\rRegularized\rInformation efficient\r\r\r\rSpeed:\r\\(10\\times\\) number of grid points\rOne run\r\rAutomated:\rNo\rYes\r\rUser knowledge:\rSome\rNone\r\rRisks:\rOver and underfitting\rNone\r\rOptimal:\rClose\rYes\r\r\r\r\rNew possibilities\r\rUsers without deep ML knowledge. \rBuild lots of models, very fast.\r\rAndrew Ng: “Anything a typical person can do with one second of thought, we can probably now or soon automate.”\r\rOnline learning.\rInner optimization routines (Gaussian Markov random fields etc.).\n\r\r\rReferences\rAkaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” IEEE Transactions on Automatic Control 19 (6). Ieee: 716–23.\n\rKonishi, Sadanori, and Genshiro Kitagawa. 1996. “Generalised Information Criteria in Model Selection.” Biometrika 83 (4). Oxford University Press: 875–90.\n\rMurata, Noboru, Shuji Yoshizawa, and Shun-ichi Amari. 1994. “Network Information Criterion-Determining the Number of Hidden Units for an Artificial Neural Network Model.” IEEE Transactions on Neural Networks 5 (6). IEEE: 865–72.\n\rShimodaira, Hidetoshi, and Haruyoshi Maeda. 2018. “An Information Criterion for Model Selection with Missing Data via Complete-Data Divergence.” Annals of the Institute of Statistical Mathematics 70 (2). Springer: 421–38.\n\rTakeuchi, Kei. 1976. “Distribution of Information Statistics and Validity Criteria of Models.” Mathematical Science 153: 12–18.\n\r\r\r\r","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"a18291993562c60e75963f77cbb3d602","permalink":"/slides/mlmeetup/mlmeetup_presentation_bl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/mlmeetup/mlmeetup_presentation_bl/","section":"slides","summary":"Outline\r\rBackground\r\rSupervised learning\rGradient boosting\r\rInformation efficient GTB\r\rFrequency domain\rJapanese tricks\rBottoms up\r\rExperimental illustrations\r\rBias\rConvergence\rComparisons\r\rRecap and possibilities\r\r\rBackground\rSupervised learning\r\r\rSupervised learning\r\r\rMapping \\(f:A \\rightarrow B\\) using a finite dataset.\rWhat is a good model?\r\rA model that generalises well to unseen data.\r\r\rGeneralises: average over new data\r\r\rwell: minimises Loss\r\r\\(\\tilde{f} = \\arg\\min_f {\\mathbb{E}}_{\\hat{w}}{\\mathbb{E}}_{{\\mathbf{x}},{y}}\\left[{\\mathcal{L}}\\left({y}, f({\\mathbf{x}}; \\hat{w})\\right)\\right]\\)\r\r\r\rGradient boosting\rAlgorithm:","tags":null,"title":"Information efficient gradient tree boosting","type":"slides"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]