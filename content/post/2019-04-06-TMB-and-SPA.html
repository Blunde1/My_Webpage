---
title: TMB and the Saddlepoint Approximation
author: Berent Lunde
date: "2019-04-06"
slug: TMB-and-the-Saddlepoint-Approximation
categories:
  - R
tags:
  - R Markdown
output:
  html_document:
    toc: true
    toc_depth: 3
bibliography: Bibliography.bib
---

<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<p> </p>
<p>We seek to employ the framework of the R package Template Model Builder <a href="https://github.com/kaskr/adcomp"><strong>TMB</strong></a> <span class="citation">(Kristensen et al. 2016)</span> which approximately “integrates out” latent variables using the Laplace approximation – to automatically solve the inner problem of the saddlepoint approximation (SPA) and return the negative logarithm of the SPA. The table of contents gives an idea of how the document is laid out. And all code can be found <a href="https://github.com/Blunde1/spaTMB">in this Github repository</a>.</p>
<div id="introduction-to-the-laplace-approximation" class="section level3">
<h3>Introduction to the Laplace approximation</h3>
<p>We consider the Laplace approximation in the context of likelihood estimation, following (read: stolen from) <span class="citation">(Kristensen et al. 2016)</span> completely: Let <span class="math inline">\(f(\mathbf{u},\theta)\)</span> denote the negative joint log-likelihood of the data and the random effects. This depends on the unknown random effects <span class="math inline">\(\mathbf{u}\in\mathbb{R}^n\)</span> and parameters <span class="math inline">\(\theta\in\mathbb{R}^m\)</span>. The maximum likelihood estimate for θ maximizes <span class="math display">\[
\begin{align*}
    L(\theta) = \int_{\mathbb{R}^n} \exp(-f(\mathbf{u},\theta)) d\mathbf{u},
\end{align*}
\]</span> i.e., the random effects <span class="math inline">\(\mathbf{u}\)</span> have been integrated out. High-dimensional integration is, in general, difficult, but can be achieved by applying the Laplace approximation, giving the approximation <span class="math display">\[
\begin{align*}
    L^*(\theta) = \frac{\sqrt{2\pi}^n \exp(-f(\mathbf{{\hat{\mathbf{u}}(\theta)}},\theta))}{\sqrt{\| \nabla_\mathbf{u}^2f({\hat{\mathbf{u}}(\theta)},\theta) \|}},   
\end{align*}
\]</span> where</p>
<ul>
<li><span class="math inline">\({\hat{\mathbf{u}}(\theta)}= \arg\min_\mathbf{u} f(\mathbf{u},\theta)\)</span>,</li>
<li>and <span class="math inline">\(\nabla_\mathbf{u}^2f(\mathbf{u},\theta)\)</span> is the Hessian matrix, denoted <span class="math inline">\(H(\theta)\)</span> from here on out.</li>
</ul>
</div>
<div id="introduction-to-the-saddlepoint-approximation" class="section level3">
<h3>Introduction to the Saddlepoint approximation</h3>
<p>The SPA takes as a vantage point the following inversion integral: <span class="math display">\[
\begin{align*}
    f_\mathbf{Y}(\mathbf{y};\theta) = \frac{1}{(2\pi)^n} \int_{\mathbb{R}^n} e^{( (\tau + i \mathbf{s})^T  \mathbf{y} ) } 
    M_\mathbf{Y}(\tau + i\mathbf{s} ; \theta) d\mathbf{s}.
\end{align*}
\]</span> where <span class="math inline">\(i=\sqrt{-1}\)</span>, <span class="math inline">\(M_\mathbf{Y}(\mathbf{s}) = E(\exp(\mathbf{s}^T\mathbf{y}))\)</span> is the moment generating function (MGF), <span class="math inline">\(\tau\in\mathbb{R}^n\)</span> such that <span class="math inline">\(E(\exp(\tau^T\mathbf{y})) &lt; \infty\)</span> <span class="citation">(Butler 2007)</span>.</p>
<p>Applying the Laplace approximation to this integral, we obtain multivaraite saddlepoint approximation: <span class="math display">\[
\texttt{spa}(f;\mathbf{y}) = 
\frac{\exp\left(K_\mathbf{y}({\hat{\mathbf{s}}(\theta)};\theta)-{\hat{\mathbf{s}}(\theta)}^T \mathbf{y}\right)}
{(2\pi)^{(n/2)}\sqrt{\| \nabla_\mathbf{s}^2 K_\mathbf{y}({\hat{\mathbf{s}}(\theta)};\theta) \|}}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(K_\mathbf{y}(\mathbf{s};\theta) = \log M_\mathbf{y}(\mathbf{s};\theta)\)</span>,</li>
<li><span class="math inline">\(\nabla_\mathbf{s}^2 K_\mathbf{y}(\mathbf{s};\theta)\)</span> is the Hessian matrix,</li>
<li>and the saddlepoint <span class="math inline">\({\hat{\mathbf{s}}(\theta)}\)</span> solves <span class="math inline">\({\hat{\mathbf{s}}(\theta)}= \arg\min_{\mathbf{s}} K_\mathbf{y}(\mathbf{s};\theta) - \mathbf{s}^T\mathbf{y}\)</span></li>
</ul>
<p>see <span class="citation">(Kleppe and Skaug 2008)</span> for a very general derivation, or the standard reference <span class="citation">(Butler 2007)</span>.</p>
</div>
<div id="introduction-to-tmb" class="section level3">
<h3>Introduction to <strong>TMB</strong></h3>
<p>Template model builder, or <strong>TMB</strong> for short, wraps (on the C++ side) the high-performance C++ libraries <strong>Eigen</strong> (a C++ template library for linear algebra: matrices, vectors, numerical solvers, and related algorithms) and <strong>CppAD</strong> (templated automatic differentiation using operator overloading), for R users to obtain highly efficient evaluations of objective functions and their derivatives (gradient and Hessian) and with automatically performing the Laplace approximation as its high-point.</p>
<p>The <strong>TMB</strong> user only has to specify the objective function <span class="math inline">\(f(\theta)\)</span>, or <span class="math inline">\(f(\mathbf{u}, \theta)\)</span> in the case of latent variables <span class="math inline">\(\mathbf{u}\)</span> that needs to be integrated out, and <strong>TMB</strong> takes care of the rest. Specifically, the (negative log) Laplace approximation to the negative joint log-likelihood is returned: <span class="math display">\[
 - \log L^*(\theta) = -n \log \sqrt{2\pi} + \frac{1}{2}\log \| H(\theta) \| + f({\hat{\mathbf{u}}(\theta)},\theta).
\]</span> Using this, the likelihood estimates of <span class="math inline">\(\theta\)</span>, i.e. <span class="math inline">\(\hat{\theta}=\arg\min_\theta - \log L^*(\theta)\)</span>, can be found very efficiently, because <strong>TMB</strong> also returnes the gradient and (finite difference approximation) Hessian of <span class="math inline">\(-\log L^*(\theta)\)</span> w.r.t. <span class="math inline">\(\theta\)</span>.</p>
<div id="example-from-tmb" class="section level4">
<h4>Example from TMB</h4>
<p><details> <summary> R code</summary></p>
<pre class="r"><code>## Simulate data
set.seed(123)
n &lt;- 10000
sigma &lt;- 0.3
phi &lt;- 0.8
simdata &lt;- function() {
    u &lt;- numeric(n)
    u[1] = rnorm(1)
    if (n &gt;= 2) 
        for (i in 2:n) {
            u[i] = phi * u[i - 1] + rnorm(1, sd = sqrt(1 - phi^2))
        }
    u &lt;- u * sigma
    x &lt;- as.numeric(rbinom(n, 1, plogis(u)))
    data &lt;- list(obs = x)
    data
}
## 
data &lt;- simdata()
parameters &lt;- list(phi = phi, logSigma = log(sigma))

## Adapt parameter list
parameters$u &lt;- rep(0, n)

require(TMB)
compile(&quot;laplace.cpp&quot;)
dyn.load(dynlib(&quot;laplace&quot;))

obj &lt;- MakeADFun(data, parameters, random = &quot;u&quot;, DLL = &quot;laplace&quot;)
obj$fn()

system.time(opt &lt;- nlminb(obj$par, obj$fn, obj$gr))
(sdr &lt;- sdreport(obj))</code></pre>
<p></details> <details> <summary> C++ code</summary></p>
<pre class="c"><code>#include &lt;TMB.hpp&gt;

template&lt;class Type&gt;
Type objective_function&lt;Type&gt;::operator() ()
{
  DATA_VECTOR(obs);
  PARAMETER(phi);
  PARAMETER(logSigma);
  PARAMETER_VECTOR(u);
  using namespace density;
  Type sigma= exp(logSigma);
  Type f = 0;
  f += SCALE(AR1(phi), sigma)(u);
  vector&lt;Type&gt; p = invlogit(u);
  f -= dbinom(obs, Type(1), p, true).sum();
  return f;
}
</code></pre>
<p></details></p>
</div>
</div>
<div id="using-tmb-in-numerical-spa-calculations-and-parameter-optimization" class="section level3">
<h3>Using <strong>TMB</strong> in numerical SPA calculations and parameter optimization</h3>
<p>Set the goal of making <strong>TMB</strong> automatically solve the inner problem of the SPA, and returning the negative log SPA for the purpose of likelihood optimisation. Ideally, we would have <strong>TMB</strong> return <span class="math display">\[
    -\log \texttt{spa}(f,\mathbf{y})
    = \frac{1}{2}\left( \log \| \nabla_\mathbf{s}^2 K_\mathbf{y}({\hat{\mathbf{s}}(\theta)};\theta) \| + n\log(2\pi) \right) - 
    \left(K_\mathbf{y}({\hat{\mathbf{s}}(\theta)};\theta)-{\hat{\mathbf{s}}(\theta)}^T \mathbf{y}\right).
\]</span> However, to solve the inner problem of the SPA, we need to supply the following objective: <span class="math display">\[
\begin{align*}
f(\mathbf{u},\theta) &amp;= K_\mathbf{y}(\mathbf{s};\theta) - \mathbf{s}^T\mathbf{y} - n\log (2\pi)\\
&amp;\sim K_\mathbf{y}(\mathbf{s};\theta) - \mathbf{s}^T\mathbf{y},
\end{align*}
\]</span> where <span class="math inline">\(\mathbf{s}\)</span> is set to <code>random</code> (and hence, will be integrated out by <strong>TMB</strong>). However, <strong>TMB</strong> would then return <span class="math display">\[
- \log L^*(\theta) = -\log \texttt{spa}(f,\mathbf{y}) + 2 \left(K_\mathbf{y}({\hat{\mathbf{s}}(\theta)};\theta)-{\hat{\mathbf{s}}(\theta)}^T \mathbf{y}\right).
\]</span> i.e., the sign is reversed on <span class="math inline">\(f(\mathbf{u},\theta) \sim K_\mathbf{y}({\hat{\mathbf{s}}(\theta)};\theta)-{\hat{\mathbf{s}}(\theta)}^T \mathbf{y}\)</span>. This is because, for the SPA, the Laplace approximation is calculated w.r.t. the objective <span class="math display">\[
f(\mathbf{u},\theta) = K_\mathbf{y}(\mathbf{\tau} + i\mathbf{s};\theta) - (\mathbf{\tau} + i\mathbf{s})^T\mathbf{y} - n\log (2\pi),
\]</span> where the imaginary unit, <span class="math inline">\(i\)</span>, reverses the sign of the Hessian (w.r.t <span class="math inline">\(\mathbf{\tau}\)</span>), replacing minimization with maximization in the the inner optimization and vice versa. <strong>TMB</strong> does not directly allow complex AD data types (while still possible inside TMB, see e.g. <a href="https://github.com/Blunde1/it-ift/blob/master/implementation/includes/complex.hpp">this file on Github</a> and example use (advanced SPA calculations) in corresponding repository).</p>
<p>There are at least four possible solutions:</p>
<ul>
<li>Use the <code>autodiff</code> namespace to construct structs of <span class="math inline">\(K\)</span> and the inner problem, and a Newton type algorithm to solve it. This is not ideal, because all Newton iterations will be taped by <strong>TMB</strong>, while only the last two should be sufficient for derivatives w.r.t. <span class="math inline">\(\theta\)</span>. See <a href="https://github.com/Blunde1/HyperSDE/blob/master/HyperSDE/src/HyperSDE.cpp">this code for an example</a>.</li>
<li>Use <code>ADREPORT</code> on the quantities needed to build the negative <span class="math inline">\(\log \texttt{spa}\)</span> and its gradient</li>
<li>Use the quantities in <code>obj$env</code> to build new <code>obj$fn</code> and <code>obj$gr</code>.</li>
<li>Update <code>MakeADFun</code>with a <code>spa=TRUE</code> option, such that SPA calculations are performed instead of Laplace. This would essentially do the same as point three (but be under the hood).</li>
</ul>
<p>I have done a hybrid between point 3 and 4; ideally I would like to make a branch on my TMB installation to do point 4, but I am on a Windows machine and hence not blessed with the developer version. If this works, I will still make a pull-request (however, untried) on MakeADFun with a SPA option to adcomp/TMB.</p>
<p>To update <code>MakeADFun</code> or the <code>obj$env</code>, the <a href="https://github.com/kaskr/adcomp/blob/master/TMB/R/TMB.R">source code</a> must be studied. The following table summarizes my findings (conditioned on no special options evoked in the <code>MakeADFun</code> call). Here, we write <span class="math inline">\(l\)</span> for the Laplace approximation and <span class="math inline">\(h\)</span> for the functional form (but unsolved inner problem), denote partial derivatives by subscripts, i.e. <span class="math inline">\(H=f_{\mathbf{u}\mathbf{u}}\)</span>, and write <span class="math inline">\(\mathbf{\nu}=(\mathbf{u},\theta)\)</span> for the collection of parameters.</p>
<table>
<colgroup>
<col width="15%" />
<col width="11%" />
<col width="33%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th>Function name</th>
<th>Order</th>
<th>Mathemtical expression</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>f</code></td>
<td><code>order==0</code></td>
<td><span class="math inline">\(f(\mathbf{u},\theta)\)</span></td>
<td>The user supplied objective</td>
</tr>
<tr class="even">
<td></td>
<td><code>order==1</code></td>
<td><span class="math inline">\(f_\mathbf{\nu}(\mathbf{u},\theta)\)</span></td>
<td>The gradient of user objective</td>
</tr>
<tr class="odd">
<td></td>
<td><code>order==2</code></td>
<td><span class="math inline">\(f_{\mathbf{\nu} \mathbf{\nu}}(\mathbf{u},\theta)\)</span></td>
<td>The Hessian matrix of <span class="math inline">\(f\)</span> w.r.t. all parameters</td>
</tr>
<tr class="even">
<td><code>h</code></td>
<td><code>order==0</code></td>
<td><span class="math inline">\(h(\mathbf{u},\theta)=f(\mathbf{u},\theta) + \frac{1}{2}\log\| f_{\mathbf{u}\mathbf{u}}(\mathbf{u},\theta) \| - \frac{n}{2}\log(2\pi)\)</span></td>
<td>The Laplace objective</td>
</tr>
<tr class="odd">
<td></td>
<td><code>order==1</code></td>
<td><span class="math inline">\(h_\nu(\mathbf{u},\theta)=f_\mathbf{\nu}(\mathbf{u},\theta) + \frac{1}{2}\nabla_\nu \log\|f_{\mathbf{u}\mathbf{u}}(u,\theta) \|\)</span></td>
<td>The derivative of the Laplace objective w.r.t. all parameters</td>
</tr>
<tr class="even">
<td><code>ff</code></td>
<td><code>order==0</code></td>
<td><span class="math inline">\(l({\hat{\mathbf{u}}(\theta)},\theta)=h({\hat{\mathbf{u}}(\theta)},\theta);~ {\hat{\mathbf{u}}(\theta)}=\arg\min_\mathbf{u} f(\mathbf{u},\theta)\)</span></td>
<td><code>h(order=0)</code> at <span class="math inline">\(\mathbf{u}={\hat{\mathbf{u}}(\theta)}\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td><code>order==1</code></td>
<td><span class="math inline">\(\begin{align}l_\theta({\hat{\mathbf{u}}(\theta)},\theta) &amp;= h_\theta({\hat{\mathbf{u}}(\theta)},\theta)\\ &amp;-h_\mathbf{u}({\hat{\mathbf{u}}(\theta)},\theta)f_{\mathbf{u}\mathbf{u}}({\hat{\mathbf{u}}(\theta)},\theta)^{-1} f_{\mathbf{u}\theta}({\hat{\mathbf{u}}(\theta)},\theta)\end{align}\)</span></td>
<td>Gradient of the Laplace approximation w.r.t. <span class="math inline">\(\theta\)</span></td>
</tr>
</tbody>
</table>
<p><details> <summary> A side note </summary> The expression <span class="math inline">\(\nabla_\nu \log\|f_{\mathbf{u}\mathbf{u}}(u,\theta) \|\)</span> in $<code>h(order==1)</code> is difficult and deserve special attention. From <span class="citation">(Searle, Casella, and McCulloch 2009)</span> page 457 or <span class="citation">(Kristensen et al. 2016)</span> eq.8, we have for an individual element <span class="math inline">\(\nu_j\)</span></p>
<p><span class="math display">\[
\frac{\partial}{\partial \nu_j} \log\|f_{\mathbf{u}\mathbf{u}}(u,\theta) \| = \mathrm{tr}\left( f_{\mathbf{u}\mathbf{u}}(u,\theta)^{-1} \frac{\partial}{\partial \nu_j} f_{\mathbf{u}\mathbf{u}}(u,\theta) \right).
\]</span> This can also be solved purely by using AD, see Table 1 in <span class="citation">(Skaug and Fournier 2006)</span>.</p>
<p></details></p>
<p>From this summary, it should be clear that if we</p>
<ol style="list-style-type: decimal">
<li>Supply the objective function <span class="math display">\[f(\mathbf{u},\theta) = K_\mathbf{y}(\mathbf{s};\theta) - \mathbf{s}^T\mathbf{y} - n\log (2\pi)\]</span> With <span class="math inline">\(\mathbf{s}\)</span> set to <code>random</code>.</li>
<li>Manipulate <code>h(order=0)</code> to return the negative log SPA <span class="math display">\[h(\mathbf{u},\theta)= -f(\mathbf{u},\theta) + \frac{1}{2}\log\| f_{\mathbf{u}\mathbf{u}}(\mathbf{u},\theta) \| - \frac{n}{2}\log(2\pi)\]</span>.</li>
<li>Manipulate <code>h(order=1)</code> to the negative log spa derivative: <span class="math display">\[
\begin{align}
h_\nu(\mathbf{u},\theta)= - f_\mathbf{\nu}(\mathbf{u},\theta) + 
\frac{1}{2}\nabla_\nu \log\|f_{\mathbf{u}\mathbf{u}}(u,\theta) \|
\end{align}
\]</span></li>
</ol>
<p>then <strong>TMB</strong> will take care of the rest. The R code for the “fix” is provided below.</p>
<p><details> <summary> R code for solution </summary></p>
<pre class="r"><code># after MakeADFun
library(Matrix)
attach(obj$env)
# update h
obj$env$h &lt;- function(theta = par, order = 0, hessian, L, ...) {
    if (order == 0) {
        ## logdetH &lt;- determinant(hessian)$mod
        logdetH &lt;- 2 * determinant(L)$mod
        ans &lt;- -f(theta, order = 0) + 0.5 * logdetH - length(random)/2 * log(2 * 
            pi)  #### updated
        if (LaplaceNonZeroGradient) {
            grad &lt;- f(theta, order = 1)[random]
            ans - 0.5 * sum(grad * as.numeric(solveCholesky(L, grad)))
        } else ans
    } else if (order == 1) 
        {
            if (LaplaceNonZeroGradient) 
                stop(&quot;Not correct for LaplaceNonZeroGradient=TRUE&quot;)
            ## browser()
            e &lt;- environment(spHess)
            solveSubset &lt;- function(L) .Call(&quot;tmb_invQ&quot;, L, PACKAGE = &quot;TMB&quot;)
            solveSubset2 &lt;- function(L) .Call(&quot;tmb_invQ_tril_halfdiag&quot;, L, PACKAGE = &quot;TMB&quot;)
            ## FIXME: The following two lines are not efficient: 1. ihessian &lt;-
            ## tril(solveSubset(L)) 2. diag(ihessian) &lt;- .5*diag(ihessian) Make option to
            ## solveSubset to return lower triangular part with diagonal halved. As it is
            ## now the output of solveSubset is symm _with upper storage_ (!) (side
            ## effect of cholmod_ptranspose) therefore tril takes long time. Further,
            ## &#39;diag&lt;-&#39; is too slow.  FIXED! :
            ihessian &lt;- solveSubset2(L)
            ## Profile case correction (1st order case only)
            if (!is.null(profile)) {
                ## Naive way: ihessian[profile,] &lt;- 0 ihessian[,profile] &lt;- 0 However, this
                ## would modify sparseness pattern and also not account for &#39;ihessian&#39; being
                ## permuted:
                perm &lt;- L@perm + 1L
                ihessian &lt;- .Call(&quot;tmb_sparse_izamd&quot;, ihessian, profile[perm], 
                  0, PACKAGE = &quot;TMB&quot;)
            }
            
            ## General function to lookup entries A subset B.  lookup.old &lt;-
            ## function(A,B){ A &lt;- as(tril(A),&#39;dtTMatrix&#39;) B &lt;- as(tril(B),&#39;dtTMatrix&#39;)
            ## match(paste(A@i,A@j),paste(B@i,B@j)) } General function to lookup entries
            ## A in B[r,r] assuming pattern of A is subset of pattern of B[r,r].
            lookup &lt;- function(A, B, r = NULL) {
                A &lt;- tril(A)
                B &lt;- tril(B)
                B@x[] &lt;- seq.int(length.out = length(B@x))  ## Pointers to full B matrix (Can have up to 2^31-1 non-zeros)
                if (!is.null(r)) {
                  ## Goal is to get: B &lt;- forceSymmetric(B) B &lt;- B[r,r,drop=FALSE] However the
                  ## internal Matrix code for &#39;B[r,r,drop=FALSE]&#39; creates temporary &#39;dgCMatrix&#39;
                  ## thereby almost doubling the number of non-zeros. Need solution that works
                  ## with max (2^31-1) non-zeros:
                  B &lt;- .Call(&quot;tmb_half_diag&quot;, B, PACKAGE = &quot;TMB&quot;)
                  B &lt;- tril(B[r, r, drop = FALSE]) + tril(t(B)[r, r, drop = FALSE])
                }
                m &lt;- .Call(&quot;match_pattern&quot;, A, B, PACKAGE = &quot;TMB&quot;)  ## Same length as A@x with pointers to B@x
                B@x[m]
            }
            if (is.null(e$ind1)) {
                ## hessian: Hessian of random effect part only.  ihessian: Inverse subset of
                ## hessian (same dim but larger pattern!).  Hfull: Pattern of full hessian
                ## including fixed effects.
                if (!silent) 
                  cat(&quot;Matching hessian patterns... &quot;)
                iperm &lt;- invPerm(L@perm + 1L)
                e$ind1 &lt;- lookup(hessian, ihessian, iperm)  ## Same dimensions
                e$ind2 &lt;- lookup(hessian, e$Hfull, random)  ## Note: dim(Hfull)&gt;dim(hessian) !
                if (!silent) 
                  cat(&quot;Done\n&quot;)
            }
            w &lt;- rep(0, length = length(e$Hfull@x))
            w[e$ind2] &lt;- ihessian@x[e$ind1]
            ## Reverse mode evaluate ptr in rangedirection w now gives .5*tr(Hdot*Hinv)
            ## !!  return
            as.vector(-f(theta, order = 1)) + .Call(&quot;EvalADFunObject&quot;, e$ADHess$ptr, 
                theta, control = list(order = as.integer(1), hessiancols = as.integer(0), 
                  hessianrows = as.integer(0), sparsitypattern = as.integer(0), 
                  rangecomponent = as.integer(1), rangeweight = as.double(w), 
                  dumpstack = as.integer(0), doforward = as.integer(1)), PACKAGE = DLL)
        }  ## order == 1
 else stop(sprintf(&quot;&#39;order&#39;=%d not yet implemented&quot;, order))
}  ## end{ h }</code></pre>
<p></details></p>
</div>
<div id="spatmb-example" class="section level3">
<h3><strong>spaTMB</strong> example</h3>
<p>Finally, an example for which we know the true solution is supplied: The Gaussian case.</p>
<p>First, we need to specify the objective function. That is, of course, the inner problem with a normalizing constant:</p>
<pre class="c"><code>#include &lt;TMB.hpp&gt;

template&lt;class Type&gt;
Type objective_function&lt;Type&gt;::operator() ()
{
    DATA_VECTOR(y);
    PARAMETER(mu);
    PARAMETER(logSigma);
    PARAMETER_VECTOR(s); // saddlepoints
    
    // Return K_y(s)-s^T y - n/2 log(2*pi)
    // K_y(s) = sum K_{y[i]}(s[i])
    // K_N(mu, logSigma)(s) = mu*s + sigma^2 * s^2 / 2
    
    Type sigma = exp(logSigma);
    int n = y.size();
    
    // Build CGF
    Type K = 0;
    for(int i=0; i&lt;n; i++){
        K += mu * s(i) + 0.5*s(i)*s(i)*sigma*sigma;
    }
    
    // Build inner problem with normalization constant
    Type res = K - (s*y).sum() - n * log(2*M_PI);
    
    // report sigma
    ADREPORT(sigma);
    
    return res;
}</code></pre>
<p>On the R side, we do what is described in the previous section:</p>
<pre class="r"><code># Simulate data
set.seed(123)
n &lt;- 10000
mu &lt;- 3
sigma &lt;- 1.5
y &lt;- rnorm(n, mu, sigma)

# data and parameters
data &lt;- list(y=y)
parameters &lt;- list(mu=0, logSigma = log(1), s=numeric(n))

# Compile c++ code and load into R
library(TMB)
compile(&quot;gauss_spa_example.cpp&quot;)</code></pre>
<p>[1] 0</p>
<pre class="r"><code>dyn.load(dynlib(&quot;gauss_spa_example&quot;))

# create adfun, set s=&quot;random&quot; for SPA inner problem
obj &lt;- MakeADFun(data, parameters, random=&quot;s&quot;, DLL=&quot;gauss_spa_example&quot;, silent = T)

# update obj$env$e functions to calculate SPA
source(&quot;spaTMB.R&quot;)

# optimize
opt &lt;- nlminb(obj$par, obj$fn, obj$gr)
rep &lt;- sdreport(obj)
knitr::kable(rbind(summary(rep, &quot;fixed&quot;, p.value = TRUE), 
                   summary(rep, &quot;report&quot;, p.value = TRUE))) %&gt;%
  kable_styling(full_width = F)</code></pre>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
Estimate
</th>
<th style="text-align:right;">
Std. Error
</th>
<th style="text-align:right;">
z value
</th>
<th style="text-align:right;">
Pr(&gt;|z^2|)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
mu
</td>
<td style="text-align:right;">
2.9964425
</td>
<td style="text-align:right;">
0.0149788
</td>
<td style="text-align:right;">
200.04556
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
logSigma
</td>
<td style="text-align:right;">
0.4040508
</td>
<td style="text-align:right;">
0.0070711
</td>
<td style="text-align:right;">
57.14143
</td>
<td style="text-align:right;">
0
</td>
</tr>
<tr>
<td style="text-align:left;">
sigma
</td>
<td style="text-align:right;">
1.4978800
</td>
<td style="text-align:right;">
0.0105916
</td>
<td style="text-align:right;">
141.42141
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p>The likelihood estimates are on point! The negative log SPA and its gradient are implemented correctly. Note that <strong>TMB</strong> is extremely fast, and even with 10000 “latent” variables (read: saddlepoints), the computations are done within a second on my computer. Finally, for the Gaussian case, we may calculate the saddlepoints explicitly to check that everything matches: <span class="math display">\[
\begin{align}
{\hat{\mathbf{s}}(\theta)}&amp;= \arg\min_s (K_Y(s)-sy) \\
{\hat{\mathbf{s}}(\theta)}&amp;: K_Y&#39;(s) -x = 0\\
K_Y(s) &amp;= \mu s + \frac{1}{2}s^2\sigma^2 \text{ in the Gaussian case, thus }\\
{\hat{\mathbf{s}}(\theta)}&amp;= \frac{x-\mu}{\sigma^2}
\end{align}
\]</span></p>
<pre class="r"><code># check s = normal sp and parameters
plot(summary(rep, &quot;random&quot;)[,&quot;Estimate&quot;], (y-opt$par[1])/(exp(opt$par[2]))^2,
     main=&quot;Numerical versus theoretical saddlepoints&quot;,
     xlab=&quot;Numerical&quot;, ylab=&quot;Theoretical&quot;)</code></pre>
<p><img src="/post/2019-04-06-TMB-and-SPA_files/figure-html/spaTMBexampleRes2Show-1.png" width="384" style="display: block; margin: auto;" /> and we see that the numerical quantities matches the theoretical ones.</p>
<p>This is pretty cool! We now only need to supply the inner problem of the SPA (typically very little code) and a normalization constant, and <strong>TMB</strong> will take care of everything in the fastest possible way! I will proceed to make a pull request soon.</p>
<div id="refs" class="references">
<div id="ref-butler2007saddlepoint">
<p>Butler, Ronald W. 2007. <em>Saddlepoint Approximations with Applications</em>. Cambridge University Press.</p>
</div>
<div id="ref-kleppe2008building">
<p>Kleppe, Tore Selland, and Hans J Skaug. 2008. “Building and Fitting Non-Gaussian Latent Variable Models via the Moment-Generating Function.” <em>Scandinavian Journal of Statistics</em> 35 (4). Wiley Online Library: 664–76.</p>
</div>
<div id="ref-kristensen2016tmb">
<p>Kristensen, Kasper, Anders Nielsen, Casper W Berg, Hans Skaug, and Bradley M Bell. 2016. “TMB: Automatic Differentiation and Laplace Approximation.” <em>Journal of Statistical Software</em> 70 (i05). Foundation for Open Access Statistics.</p>
</div>
<div id="ref-searle2009variance">
<p>Searle, Shayle R, George Casella, and Charles E McCulloch. 2009. <em>Variance Components</em>. Vol. 391. John Wiley &amp; Sons.</p>
</div>
<div id="ref-skaug2006automatic">
<p>Skaug, Hans J, and David A Fournier. 2006. “Automatic Approximation of the Marginal Likelihood in Non-Gaussian Hierarchical Models.” <em>Computational Statistics &amp; Data Analysis</em> 51 (2). Elsevier: 699–709.</p>
</div>
</div>
</div>
