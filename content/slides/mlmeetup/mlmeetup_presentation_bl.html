---
title: "Information efficient gradient tree boosting"
author: "Berent Ånund Strømnes Lunde"
date: "31 October 2018"
output:
  beamer_presentation: default
  ioslides_presentation:
    smaller: yes
self_contained: yes
bibliography: mlmeetup_bib.bib
---



<p>  </p>
<div id="outline" class="section level2">
<h2>Outline</h2>
<ul>
<li>Background
<ul>
<li>Supervised learning</li>
<li>Gradient boosting</li>
</ul></li>
<li>Information efficient GTB
<ul>
<li>Frequency domain</li>
<li>Japanese tricks</li>
<li>Bottoms up</li>
</ul></li>
<li>Experimental illustrations
<ul>
<li>Bias</li>
<li>Convergence</li>
<li>Comparisons</li>
</ul></li>
<li>Recap and possibilities</li>
</ul>
</div>
<div id="background" class="section level1">
<h1>Background</h1>
<div id="supervised-learning" class="section level2">
<h2>Supervised learning</h2>
<div class="columns-2">
<p><img src="figures/isl_regression.png" width="400px" height="400px" /></p>
<p><img src="figures/loss_vs_complexity.png" width="400px" height="400px" /></p>
</div>
</div>
<div id="supervised-learning-1" class="section level2">
<h2>Supervised learning</h2>
<blockquote>
<ul>
<li>Mapping <span class="math inline">\(f:A \rightarrow B\)</span> using a finite dataset.</li>
<li>What is a good model?</li>
<li><ul>
<li>A model that generalises well to unseen data.</li>
</ul></li>
<li><ul>
<li><em>Generalises</em>: average over new data</li>
</ul></li>
<li><ul>
<li><em>well</em>: minimises Loss</li>
</ul></li>
<li><span class="math inline">\(\tilde{f} = \arg\min_f {\mathbb{E}}_{\hat{w}}{\mathbb{E}}_{{\mathbf{x}},{y}}\left[{\mathcal{L}}\left({y}, f({\mathbf{x}}; \hat{w})\right)\right]\)</span></li>
</ul>
</blockquote>
</div>
<div id="gradient-boosting" class="section level2">
<h2>Gradient boosting</h2>
<p>Algorithm:</p>
<ol style="list-style-type: decimal">
<li>Initialize model with a constant value: <span class="math inline">\(F_0(x) = \underset{\gamma}{\arg\min} \sum_{i=1}^n {\mathcal{L}}(y_i, \gamma).\)</span></li>
<li>For <span class="math inline">\(m = 1\)</span> to <span class="math inline">\(M\)</span>:
<ol style="list-style-type: lower-roman">
<li>Compute so-called ‘’pseudo-residuals’’: <span class="math inline">\(r_{im} = -\left[\frac{\partial {\mathcal{L}}(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)} \quad \mbox{for } i=1,\ldots,n.\)</span></li>
<li>Fit a base learner (e.g. tree) <span class="math inline">\(h_m(x)\)</span> to pseudo-residuals, i.e. train it using the training set <span class="math inline">\(\{(x_i, r_{im})\}_{i=1}^n\)</span>.</li>
<li>Compute multiplier <span class="math inline">\(\gamma_m\)</span> by solving the following one-dimensional optimization problem: <span class="math inline">\(\gamma_m = \underset{\gamma}{\operatorname{arg\,min}} \sum_{i=1}^n {\mathcal{L}}\left(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)\right)\)</span>.</li>
<li>Update the model: <span class="math inline">\(F_m(x) = F_{m-1}(x) + \gamma_m h_m(x).\)</span></li>
</ol></li>
<li>Output <span class="math inline">\(F_M(x)\)</span>.</li>
</ol>
</div>
<div id="gradient-tree-boosting" class="section level2">
<h2>Gradient tree boosting</h2>
<p>At iteration <span class="math inline">\(\small m+1\)</span>:</p>
<ul>
<li>Computes <span class="math inline">\(\small g_i=-r_i\)</span> and <span class="math inline">\(\small h_i=\left[\frac{\partial^2 {\mathcal{L}}(y_i, F(x_i))}{\partial F(x_i)^2}\right]_{F(x)=F_{m-1}(x)}\quad \mbox{for } i=1,\ldots,n.\)</span></li>
<li>Approximates <span class="math inline">\({\mathcal{L}}\)</span> by a second order approximation about <span class="math inline">\(F_{m}\)</span>: <span class="math display">\[{\small
{\mathbb{E}}_{{\mathbf{x}},{y}}\left[ {\mathcal{L}}({y}, (f_m+f)({\mathbf{x}}) \right] 
\approx
\frac{1}{n}\sum_{i=1}^{n} 
{\mathcal{L}}(y_i,f_m({\mathbf{x}}_i))+g_i f({\mathbf{x}}_i)
+\frac{1}{2}h_if({\mathbf{x}}_i)^2
}\]</span></li>
<li>Solves the quadratic problem exactly: <span class="math inline">\(\small \hat{w} = -\frac{\sum g_i}{\sum h_i}\)</span> and <span class="math inline">\(\small {\mathcal{L}}(y,\hat{w}) = -\frac{\left(\sum g_i\right)^2}{2\sum h_i}\)</span></li>
<li>Splits nodes to maximize loss reduction:
<center>
<span class="math inline">\({\small \frac{1}{2}\left[\frac{\left(\sum_{i\in I_L(j,s)}g_i\right)^2}{\sum_{i\in I_L(j,s)}h_i} + \frac{\left(\sum_{i\in I_R(j,s)}g_i\right)^2}{\sum_{i\in I_R(j,s)}h_i} -\frac{\left(\sum_{i\in I_k}g_i\right)^2}{\sum_{i\in I_k}h_i}\right] }\)</span>
</center></li>
</ul>
</div>
<div id="the-problem" class="section level2">
<h2>The problem</h2>
<ul>
<li>Wrong objective:<br />
<span class="math inline">\({\mathcal{L}}(y,\hat{w}) = -\frac{\left(\sum g_i\right)^2}{2\sum h_i}\)</span> is not an unbiased estimator of <span class="math inline">\({\mathbb{E}}_{\hat{w}}{\mathbb{E}}_{{\mathbf{x}},{y}}\left[{\mathcal{L}}\left({y}, f({\mathbf{x}}; \hat{w})\right)\right]\)</span></li>
<li>Regularization: <span class="math inline">\({\mathcal{L}}+ \Omega(w)\)</span></li>
<li>XGBoost and LightGBM each have &gt; 10 hyperparameters that should be configured.</li>
<li>High dimensional optimisation is difficult.</li>
<li>Time consuming.</li>
<li>Requires human expertiese.</li>
<li>Cross validation is information inefficient.
<ul>
<li>Variable results.</li>
<li>Risky in terms of over and underfitting.</li>
<li>Kaggle mantra: trust your local validation…</li>
</ul></li>
</ul>
</div>
</div>
<div id="information-efficient-gradient-tree-boosting" class="section level1">
<h1>Information efficient gradient tree boosting</h1>
<div id="retrieval-of-the-estimator-distribution" class="section level2">
<h2>Retrieval of the estimator distribution</h2>
<p>For model selection purposes:</p>
<ul>
<li>Seek the distribution of <span class="math inline">\(\hat{w} = -\frac{\sum g_i}{\sum h_i}\)</span>.</li>
</ul>
<p>However:</p>
<ol style="list-style-type: decimal">
<li>Distribution is unknown unless prior information about the data-generating process is known.</li>
<li>Asymptotic theory assumptions does not hold in tree-leaves.</li>
<li>Bootstrapping to estimate the empirical/ resample distribution is extremely costly.</li>
</ol>
<ul>
<li>But we know the probability of resampling one observation event: <span class="math inline">\(\frac{1}{n}\)</span></li>
<li>We also know the one-observation resample events characteristic function (Fourier transform): <span class="math display">\[\small 
{\varphi^*}_{g,h}(u,v) = \frac{1}{n} \sum_{j=1}^n e^{iug_j + ivh_j}\]</span></li>
</ul>
</div>
<div id="the-frequency-domain" class="section level2">
<h2>The frequency domain</h2>
<ul>
<li>The frequency domain is a fantastic place to combine randomness</li>
</ul>
<p>The characteristic function takes a frequency, <span class="math inline">\(s\)</span>, as argument: <span class="math display">\[\small
        \varphi_X(s) = {\mathbb{E}}\left[e^{isx}\right] = \int e^{isx}p_X(x)~dx\]</span></p>
<ul>
<li><span class="math inline">\(s\)</span> is the <em>angular frequency</em> <span class="math inline">\(s=2\pi \tau\)</span>, <span class="math inline">\(\tau\)</span> is the number of rotations per unit of time.</li>
<li>The average of <span class="math inline">\(X\)</span> wrapped around the unit circle with <em>angular frequency</em> <span class="math inline">\(s\)</span>.</li>
<li><span class="math inline">\(\varphi_X(s)\)</span> is called the <em>Fourier transform</em> of <span class="math inline">\(p_X(x)\)</span>.</li>
</ul>
</div>
<div id="the-frequency-domain-1" class="section level2">
<h2>The frequency domain</h2>
<p>The characteristic function takes a frequency, <span class="math inline">\(s\)</span>, as argument: <span class="math display">\[ \small
        \varphi_X(s) = {\mathbb{E}}\left[e^{isx}\right] = \int e^{isx}p_X(x)~dx\]</span></p>
<ul>
<li>The average of <span class="math inline">\(X\)</span> wrapped around the unit circle with <em>angular frequency</em> <span class="math inline">\(s\)</span>. <img src="figures/uniform_cf_1.png" width="550px" height="370px" style="display: block; margin: auto;" /></li>
</ul>
</div>
<div id="property-of-the-characteristic-function" class="section level2">
<h2>Property of the characteristic function</h2>
<p>General property, if <span class="math inline">\(\small X_1\)</span> and <span class="math inline">\(\small X_2\)</span> are independent, then <span class="math display">\[\small
        \varphi_{\left(X_1+X_2\right)}(s) = \varphi_{X_1}(s) \times \varphi_{X_2}(s).\]</span> Thus <span class="math display">\[
\begin{align}\label{eq empirical cf}
\varphi_{\sum g,\sum h}^*(u,v) = \left[ \varphi_{g, h}^*\left(u,v\right)  \right]^n.
\end{align}\]</span></p>
<p>We can obtain the density and distribution functions: <span class="math display">\[\small
    \begin{align*}
    p_X(x) &amp;= \frac{1}{2\pi} \int_{-\infty}^\infty
    \varphi_X(s)e^{-isx} ds.
    \end{align*}\]</span></p>
<p><img src="figures/tikz_frequency.png" width="450px" height="200px" style="display: block; margin: auto;" /></p>
</div>
<div id="back-to-the-actual-problem" class="section level2">
<h2>Back to the actual problem</h2>
<p>What we have: training error – the error on the data we trained on <span class="math display">\[\small
{\mathbb{E}}_{{\mathbf{x}},{y}}\left[ {\mathcal{L}}({y},F({\mathbf{x}};\hat{w}({\mathbf{x}},{y})))\right].\]</span></p>
<p>What we need: test error – the error on previously unseen data <span class="math display">\[\small
{\mathbb{E}}_{\hat{w}}{\mathbb{E}}_{{\mathbf{x}},{y}}\left[{\mathcal{L}}\left({y}, F({\mathbf{x}}; \hat{w})\right)\right].\]</span></p>
<p>How to get it?</p>
<p>This is a known problem!</p>
<blockquote>
<ul>
<li><span class="citation">Akaike (1974)</span>: log-likelihood, asymptotics, true model is in the modelspace.</li>
<li><span class="citation">Takeuchi (1976)</span>: generalises AIC – true model not necessarily in the modelspace.</li>
<li><span class="citation">Murata, Yoshizawa, and Amari (1994)</span>: generalises TIC – differentiable loss function</li>
<li><span class="citation">Konishi and Kitagawa (1996)</span></li>
<li><span class="citation">Shimodaira and Maeda (2018)</span>: generalises TIC to missing data</li>
</ul>
</blockquote>
</div>
<div id="an-information-criterion-for-gtb" class="section level2">
<h2>An information criterion for GTB</h2>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-7"></span>
<img src="figures/tikz_japanese.png" alt="Geometric view of Japanese tricks" width="400px" height="200px" />
<p class="caption">
Figure 1: Geometric view of Japanese tricks
</p>
</div>
<ul>
<li>Avoids asymptotics by utilizing the frequency domain.</li>
<li>Exploit the iterative structure of gradient boosting and the locally constant structure of trees.</li>
<li>Avoids high-dimensional matrix inversion (for possibly non-positive definite matrices). <span class="math display">\[\small
\begin{align}\label{eq criterion exact}
{\mathbb{E}}_{f_m}{\mathbb{E}}_{{y}} \left[   \hat{{\mathcal{L}}}(y,\hat{f}_m)  \right]
=
{\mathbb{E}}_{{y}} \left[   \hat{{\mathcal{L}}}(y,\hat{f}_m)  \right]
&amp;+  \frac{1}{2} \sum_{k=1}^{T}{\mathbb{E}}_{{y}} \left[     h| q({\mathbf{x}})=k \right  ]{\mathrm{Var}}[w_k]\notag\\
&amp;   +   \frac{1}{2} 
\sum_{k=1}^{T}{\mathbb{E}}_{{y}} \left[ \left\{ h| q({\mathbf{x}})=k \right\} (w_0 - \hat{w}_k)^2 \right].
\end{align}\]</span></li>
</ul>
</div>
<div id="bottoms-up-branch-pruning" class="section level2">
<h2>Bottoms up: branch pruning</h2>
<p><img src="figures/algo_bottomsup.png" width="500px" height="250px" /></p>
<p><img src="figures/tikz_bottomsup.png" width="700px" height="250px" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="experimental-illustrations" class="section level1">
<h1>Experimental illustrations</h1>
<div id="bias" class="section level2">
<h2>Bias</h2>
<p><img src="figures/Criterion_bias.png" width="600px" height="300px" style="display: block; margin: auto;" /></p>
<ul>
<li>Focus on the individual bias in a node.</li>
<li>Repeated simulation experiment to obtain the distribution of bias estimators.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Cross validation is information inefficient.</li>
<li>Asymptotic criteria, NIC, is biased downward.</li>
<li>LIC is slightly biased upwards.</li>
</ol>
</div>
<div id="convergence" class="section level2">
<h2>Convergence</h2>
<p><img src="figures/gtb_loss_convergence.png" width="600px" height="300px" style="display: block; margin: auto;" /></p>
<ul>
<li>Training, testing, and estimated test-loss versus number of trees added.</li>
<li>Number of leaves in n’th tree.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Convergence in number of trees: The estimted test-loss minimum correspond to the testing loss.</li>
<li>The number of leaves decreases as information is learned by the model.</li>
</ol>
</div>
<div id="comparisons-on-titanic-data" class="section level2">
<h2>Comparisons on Titanic data</h2>
<p><img src="figures/Model_loss_n30.png" width="600px" height="300px" style="display: block; margin: auto;" /></p>
<ul>
<li>Comparisons versus a regularized model and one trained with a validation dataset (30% of training).</li>
<li>Repeated random splitting in training and test datasets.</li>
</ul>
<ol style="list-style-type: decimal">
<li>Our information efficient approach is slightly better overall (mean).</li>
<li>The standard deviation is the lowest for our approach (consistency and information efficient).</li>
</ol>
</div>
</div>
<div id="recap-and-possibilities" class="section level1">
<h1>Recap and possibilities</h1>
<div id="two-different-approaches" class="section level2">
<h2>Two different approaches</h2>
<table>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Regularized</th>
<th>Information efficient</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Speed:</td>
<td><span class="math inline">\(10\times\)</span> number of grid points</td>
<td>One run</td>
</tr>
<tr class="even">
<td>Automated:</td>
<td>No</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>User knowledge:</td>
<td>Some</td>
<td>None</td>
</tr>
<tr class="even">
<td>Risks:</td>
<td>Over and underfitting</td>
<td>None</td>
</tr>
<tr class="odd">
<td>Optimal:</td>
<td>Close</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
<div id="new-possibilities" class="section level2">
<h2>New possibilities</h2>
<ul>
<li><p>Users without deep ML knowledge. <img src="figures/data_science_1.png" width="200px" height="200px" style="display: block; margin: auto;" /></p></li>
<li>Build lots of models, very fast.
<ul>
<li>Andrew Ng: “Anything a typical person can do with one second of thought, we can probably now or soon automate.”</li>
</ul></li>
<li>Online learning.</li>
<li><p>Inner optimization routines (Gaussian Markov random fields etc.).</p></li>
</ul>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-akaike1974new">
<p>Akaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” <em>IEEE Transactions on Automatic Control</em> 19 (6). Ieee: 716–23.</p>
</div>
<div id="ref-konishi1996generalised">
<p>Konishi, Sadanori, and Genshiro Kitagawa. 1996. “Generalised Information Criteria in Model Selection.” <em>Biometrika</em> 83 (4). Oxford University Press: 875–90.</p>
</div>
<div id="ref-murata1994network">
<p>Murata, Noboru, Shuji Yoshizawa, and Shun-ichi Amari. 1994. “Network Information Criterion-Determining the Number of Hidden Units for an Artificial Neural Network Model.” <em>IEEE Transactions on Neural Networks</em> 5 (6). IEEE: 865–72.</p>
</div>
<div id="ref-shimodaira2018information">
<p>Shimodaira, Hidetoshi, and Haruyoshi Maeda. 2018. “An Information Criterion for Model Selection with Missing Data via Complete-Data Divergence.” <em>Annals of the Institute of Statistical Mathematics</em> 70 (2). Springer: 421–38.</p>
</div>
<div id="ref-takeuchi1976distribution">
<p>Takeuchi, Kei. 1976. “Distribution of Information Statistics and Validity Criteria of Models.” <em>Mathematical Science</em> 153: 12–18.</p>
</div>
</div>
</div>
</div>
